{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to Lambda Forge, a cutting-edge framework designed to redefine the deployment and management of AWS Lambda functions. Lambda Forge stands at the intersection of innovation and functionality, offering a seamless pipeline for transitioning code from retrieval to production \ud83d\ude80.</p> <p>Lambda Forge is not just about deployment; it's about creating a harmonious environment for your Lambda functions. With an emphasis on a consistent directory structure, it sets the stage for effortless management, deployment, and scalability. Through the synergistic use of AWS CloudFormation stack and AWS CDK, the framework orchestrates the build, test, and deployment processes, efficiently managing code from GitHub repositories across various environments.</p> <p>One of the hallmark features of Lambda Forge is its dedication to best practices in code organization, testing, and integration. It champions the cause of high-quality, maintainable code standards \ud83d\udcda. The introduction of the Forge command-line interface tool is a game-changer, simplifying the initial setup and enabling a standardized folder structure while pre-configuring essential settings for Lambda functions.</p> <p>Moreover, Lambda Forge fosters a modular code architecture, promoting unit and integration testing. A standout feature is the automatic generation of Swagger documentation from data classes within each Lambda function, a boon for developers. This enriches the framework, offering developers the tools to produce and access comprehensive API documentation with ease, thereby enhancing the development, management, and deployment journey of Lambda functions with Lambda Forge.</p>"},{"location":"page2/","title":"Getting Started","text":""},{"location":"page2/#install-and-configure-aws-cdk","title":"Install and Configure AWS CDK","text":"<p>Lambda Forge is built on top of AWS Cloud Development Kit (CDK) and it's essential for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. Execute the following commands to install the AWS CDK globally and set up your AWS credentials:</p> <pre><code>npm install -g aws-cdk\naws configure\n</code></pre> <p>During the configuration, you will be prompted to enter your AWS Access Key ID, Secret Access Key, default region name, and output format.</p>"},{"location":"page2/#create-a-new-directory","title":"Create a new directory","text":"<pre><code>mkdir lambda_forge_demo\ncd lambda_forge_demo\n</code></pre>"},{"location":"page2/#create-a-new-virtual-environment","title":"Create a new virtual environment","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"page2/#install-lambda-forge","title":"Install lambda-forge","text":"<pre><code>pip install lambda-forge --extra-index-url https://pypi.org/simple --extra-index-url https://test.pypi.org/simple/\n</code></pre>"},{"location":"page2/#verify-installation","title":"Verify Installation","text":"<p>Having successfully installed Lambda Forge, you are now ready to explore the capabilities of the Forge Command Line Interface (CLI). Begin by entering the following command to access the comprehensive list of available options and commands:</p> <pre><code>forge --help\n</code></pre>"},{"location":"page2/#create-a-new-github-repository","title":"Create a new Github Repository","text":"<p>Lambda Forge simplifies your workflow by automatically configuring a CI/CD pipeline within a GitHub repository. Therefore, our next step is to create a new repository on GitHub. This foundational setup enables Lambda Forge to seamlessly integrate and automate the development, testing, and deployment processes for your projects.</p>"},{"location":"page2/#create-a-new-project","title":"Create a new project:","text":"<p>Let's start a new project without generating docs initially.</p> <pre><code>forge project lambda-forge-demo --repo-owner \"$GITHUB-USER\" --repo-name \"$GITHUB-REPO\" --no-docs\n</code></pre> <p>Make sure to replace $GITHUB-USER and $GITHUB-REPO with your actual GitHub username and the repository name you established in the previous step.</p>"},{"location":"page2/#project-structure","title":"Project Structure","text":"<p>Upon creatig your project, several directories and files are automatically generated for you. This initial structure is designed to streamline the setup process and provide a solid foundation for further development.</p> <p>In the upcoming sections of this tutorial, we'll explore each of these components in detail. For now, familiarize yourself with the foundational structure that should resemble the following:</p> <pre><code>.\n\u251c\u2500\u2500 authorizers\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 functions\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 infra\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 services\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 api_gateway.py\n\u2502   \u2502   \u2514\u2500\u2500 aws_lambda.py\n\u2502   \u251c\u2500\u2500 stacks\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 dev_stack.py\n\u2502   \u2502   \u251c\u2500\u2500 lambda_stack.py\n\u2502   \u2502   \u251c\u2500\u2500 prod_stack.py\n\u2502   \u2502   \u2514\u2500\u2500 staging_stack.py\n\u2502   \u2514\u2500\u2500 stages\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 deploy.py\n\u251c\u2500\u2500 .coveragerc\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 cdk.json\n\u251c\u2500\u2500 pytest.ini\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"page2/#creating-your-first-hello-world-function","title":"Creating Your First Hello World Function","text":"<p>Embarking on the journey with Lambda Forge, creating a public \"Hello World\" function is a fantastic way to get started. This function will serve as a simple demonstration of Lambda Forge's ability to quickly deploy serverless functions accessible via an HTTP endpoint. Here's how you can create your very first public Hello World function.</p> <pre><code>forge function hello_world --method \"GET\" --description \"A simple hello world\" --public\n</code></pre> <p>This command instructs Lambda Forge to generate a new lambda function named hello_world. The --method \"GET\" parameter specifies that this function will be accessible via an HTTP GET request. The --description provides a brief explanation of the function's purpose, and --public makes the function publicly accessible, allowing anyone with the URL to invoke it.</p>"},{"location":"page2/#understanding-the-function-structure","title":"Understanding the Function Structure","text":"<p>When you create a new function with Lambda Forge, it not only simplifies the creation process but also sets up a robust and organized file structure for your function. This structure is designed to support best practices in software development, including separation of concerns, configuration management, and testing. Let's break down the structure of the automatically generated hello_world function:</p> <pre><code>functions/\n\u2514\u2500\u2500 hello_world/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre> <ul> <li><code>functions/</code> This directory is the root folder for all your Lambda functions. Each function has its own subdirectory within this folder.</li> <li><code>hello_world/</code> The hello_world subdirectory contains all the necessary files for your function to run, be configured, and tested.</li> <li><code>__init__.py</code> This file marks the directory as a Python package, allowing its modules to be imported elsewhere. It's a standard practice in Python to facilitate package organization.</li> <li><code>config.py</code> Holds the configuration settings for the function. These might include environment variables, resource identifiers, and other parameters critical for the function's operation. Keeping configuration separate from code is a best practice, as it enhances maintainability and scalability.</li> <li><code>integration.py</code> Contains integration tests that simulate the interaction of your function with external services or resources. These tests ensure that your function integrates correctly with other parts of the system and external dependencies.</li> <li><code>main.py</code> This is where the core logic of your Lambda function resides. The handler function, which AWS Lambda invokes when the function is executed, is defined here. You'll implement the functionality of your \"Hello World\" response in this file.</li> <li><code>unit.py</code> Contains unit tests for your function. Unit tests focus on testing individual parts of the function's code in isolation, ensuring that each component behaves as expected. This is crucial for identifying and fixing bugs early in the development process.</li> </ul>"},{"location":"page2/#why-this-structure-matters","title":"Why This Structure Matters","text":"<p>This organized approach offers several benefits:</p> <ul> <li>Maintainability: Separating different aspects of the function (such as configuration, testing, and core logic) makes it easier to manage and update the code.</li> <li>Scalability: As your application grows, having a consistent structure across functions allows for easier scaling and integration of new features or services.</li> <li>Testing: Including both unit and integration tests from the outset encourages a testing culture, leading to more robust and reliable code.</li> </ul> <p>As you continue to develop with Lambda Forge, this structure will help keep your projects organized and maintainable, regardless of their complexity or scale.</p>"},{"location":"page2/#configuring-your-lambda-function-dependencies","title":"Configuring Your Lambda Function Dependencies","text":""},{"location":"page2/#the-services-class","title":"The Services Class","text":"<p>Within the <code>infra/services/__init__.py</code> file, you'll find the Services class, a comprehensive resource manager designed to streamline the interaction with AWS services. This class acts as a dependency injector, enabling the easy and efficient configuration of AWS resources directly from your <code>config.py</code> files.</p> infra/services/__init__.py<pre><code>from infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\n\nclass Services:\n\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n</code></pre>"},{"location":"page2/#utilizing-the-services-class-in-configpy","title":"Utilizing the Services Class in config.py","text":"<p>In our Lambda Forge projects, the <code>config.py</code> file plays a crucial role in defining and configuring the dependencies required by a Lambda function. This includes everything from AWS Lambda layers, permissions to environment variables and DynamoDB tables. By passing an instance of Services to our configuration classes, we can seamlessly create and manage resources such as Lambda functions and API Gateway endpoints.</p> functions/hello_world/config.py<pre><code>from infra.services import Services\n\nclass HelloWorldConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"HelloWorld\",\n            path=\"./functions/hello_world\",\n            description=\"A simple hello world\"\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/hello_world\", function, public=True)\n</code></pre>"},{"location":"page2/#implementing-the-lambda-function","title":"Implementing the Lambda Function","text":"<p>Your Lambda function's implementation should be in the <code>main.py</code> file. Below is an example showcasing our simple HelloWorld function:</p> functions/hello_world/main.py<pre><code>import json\nfrom dataclasses import dataclass\n\n@dataclass\nclass Input:\n    pass\n\n@dataclass\nclass Output:\n    message: str\n\ndef lambda_handler(event, context):\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"Hello World!\"})\n    }\n</code></pre> <p>The <code>Input</code> and <code>Output</code> data classes define the structure for each endpoint, laying the groundwork for generating the comprehensive Swagger documentation. This documentation precisely outlines the expected input and output data for every endpoint.</p> <p>Considering the project was initiated with the <code>--no-docs</code> flag, let's temporarily skip this documentation generation phase. However, we intend to revisit and implement documentation generation at a future point in this tutorial.</p>"},{"location":"page2/#deploying-your-lambda-function","title":"Deploying Your Lambda Function","text":"<p>To deploy your Lambda function, you should integrate the Config class within the <code>infra/stacks/lambda_stack.py</code> file.</p> <p>The Forge CLI streamlines this process by automatically incorporating it for you.</p> infra/stacks/lambda_stack.py<pre><code>from aws_cdk import Stack\nfrom constructs import Construct\nfrom infra.services import Services\nfrom lambda_forge import release\nfrom functions.hello_world.config import HelloWorldConfig\n\n\n@release\nclass LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-CDK\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # HelloWorld\n        HelloWorldConfig(self.services)\n</code></pre>"},{"location":"page2/#create-a-github-personal-access-token","title":"Create a GitHub Personal Access Token","text":"<p>Lambda Forge uses CodePipeline to interact with your GitHub repository. To enable this, generate a GitHub personal access token by following these steps:</p> <ol> <li>Navigate to \"Developer Settings\" in your GitHub account.</li> <li>Select \"Personal access tokens,\" then \"Tokens (classic).\"</li> <li>Click \"Generate new token,\" ensuring the \"repo\" scope is selected for full control of private repositories.</li> <li>Complete the token generation process.</li> </ol> <p>Your token will follow this format: <code>ghp_********************************</code></p>"},{"location":"page2/#store-the-token-on-aws-secrets-manager","title":"Store the token on AWS Secrets Manager","text":"<p>Save this token in AWS Secrets Manager as <code>plain text</code> using the precise name github-token. This specific naming is vital as it corresponds to the default identifier that the CDK looks for within your AWS account.</p>"},{"location":"page2/#push-your-code-to-github","title":"Push Your Code To Github","text":"<p>With all the required settings now in place, we're ready to upload our code to the GitHub repository.</p> <p>Lambda Forge is designed to support a multi-stage deployment process, automatically creating environments for Production, Staging and Development. These environments correspond to the <code>main</code>, <code>staging</code>, and <code>dev</code> branches, respectively.</p> <p>Let's proceed by setting up these branches:</p> <pre><code># Initialize the Git repository\ngit init\ngit add .\n\n# Commit the changes\ngit commit -m \"Initial commit\"\n\n# Set the remote repository\ngit remote add origin git@github.com:$GITHUB_USER/$GITHUB_REPO.git\n\n# Create, checkout, and push the 'main' branch\ngit checkout -b main\ngit push -u origin main\n\n# Create, checkout, and push the 'staging' branch\ngit checkout -b staging\ngit push -u origin staging\n\n# Create and push the 'dev' branch\ngit branch -M dev\ngit push -u origin dev\n</code></pre>"},{"location":"page2/#deploying-the-stacks","title":"Deploying the Stacks","text":"<p>After pushing your code to GitHub, the next step is deploying your stacks to AWS using the AWS Cloud Development Kit (CDK). Deploy your stacks by running the following commands in your terminal:</p> <pre><code>cdk synth\ncdk deploy --all --require-approval never\n</code></pre> <p>These commands kick off the creation of three separate stacks in AWS CloudFormation:</p> <ul> <li>Dev-Lambda-Forge-Demo-Stack: For the development stage.</li> <li>Staging-Lambda-Forge-Demo-Stack: For the staging stage.</li> <li>Prod-Lambda-Forge-Demo-Stack: For the production stage.</li> </ul> <p>Every resource created on AWS by Lambda Forge adhere to a convention that incorporates the deployment stage, project name and the resource name, ensuring a clear and systematic identification across the project. The project name, a central element of this naming convention, is specified in the <code>cdk.json</code> file, which Forge automatically configured.</p> cdk.json<pre><code>...\n    \"region\": \"us-east-2\",\n    \"account\": \"\",\n    \"name\": \"Lambda-Forge-Demo\",\n    \"repo\": {\n      \"owner\": \"$GITHUB-USER\",\n      \"name\": \"$GITHUB-REPO\"\n    },\n...\n</code></pre> <p>Following a successful deployment, three corresponding pipelines are automatically generated for each stage:</p> <ul> <li>Dev-Lambda-Forge-Demo-Pipeline</li> <li>Staging-Lambda-Forge-Demo-Pipeline</li> <li>Prod-Lambda-Forge-Demo-Pipeline</li> </ul> <p>You can view these pipelines by navigating to the AWS CodePipeline dashboard. </p> <p></p>"},{"location":"page2/#customizing-pipeline-steps","title":"Customizing Pipeline Steps","text":"<p>In Lambda Forge, pipelines are defined within their specific stack files located at <code>infra/stacks/dev_stack.py</code>, <code>infra/stacks/staging_stack.py</code>, and <code>infra/stacks/prod_stack.py</code>. Below is an outline of the default steps included in each pipeline, along with details on how they function and how you can interact with them.</p> <ul> <li> <p>Coverage: Measures the percentage of your production code covered by unit tests, failing if coverage falls below 80% as default. To view the coverage report, navigate to <code>Details -&gt; Reports</code> in CodePipeline.</p> </li> <li> <p>Unit Tests: Runs unit tests to validate the functionality of individual components within your code. Access the unit test report via <code>Details -&gt; Reports</code> in CodePipeline.</p> </li> <li> <p>Validate Docs: Verifies that all Lambda functions invoked by API Gateway have their Input and Output data classes correctly defined in the <code>main.py</code> file.</p> </li> <li> <p>Validate Integration Tests: This step ensures that all endpoints triggered by the API Gateway are covered by at least one integration test. To achieve this, use the custom decorator <code>@pytest.mark.integration</code> and specify the method and endpoint arguments to declare that the test covers a specific endpoint.</p> </li> <li> <p>Generate Docs: Automatically produces Swagger documentation for all API Gateway endpoints. This requires <code>Input</code> and <code>Output</code> data classes for each Lambda function in the <code>main.py</code> file. Documentation is deployed directly to API Gateway and is accessible at the <code>/docs</code> endpoint.</p> </li> <li> <p>Integration Tests: Performs integration testing to assess the system's overall functionality. Access the integration test report through <code>Details -&gt; Reports</code> in CodePipeline.</p> </li> </ul> <p>Lambda Forge provides a suggested pipeline configuration, emphasizing flexibility in its design. You're encouraged to customize these pipelines to fit your project's needs. Whether adding new steps, adjusting existing ones, or reordering them, the framework is designed to accommodate your project's specific requirements. This level of customization ensures that your pipelines align closely with your development, testing, and deployment strategies, providing a robust foundation for your application's continuous integration and delivery processes.</p> <p>After the pipelines have executed, you may notice that while the Development pipeline succeeds, the Staging and Production pipelines fail.</p> <p></p> <p>This failure is expected, It occurs because the integration test step attempts to send a GET request to the deployed Lambda function. However, since the Stack has just been created, the Lambda function's URL is not yet available, causing the test to fail.</p> functions/hello_world/integration.py<pre><code>import pytest\nimport requests\nfrom lambda_forge.constants import BASE_URL\n\n@pytest.mark.integration(method=\"GET\", endpoint=\"/hello_world\")\ndef test_hello_world_status_code_is_200():\n\n    response = requests.get(url=f\"{BASE_URL}/hello_world\")\n\n    assert response.status_code == 200\n</code></pre>"},{"location":"page2/#accessing-your-lambda-function","title":"Accessing Your Lambda Function","text":"<p>Note that the <code>Integration_Test</code> step failed after the deployment in the staging pipeline.</p> <p></p> <p>This means that both Dev and the Staging Lambda function are successfully deployed.</p> <p>In this tutorial, we will use the staging URL to test our endpoints, considering it as a pre-production layer. Feel free to utilize the development URL instead if it suits your case better.</p> <p>Navigate to the AWS Lambda section on AWS and search for the Staging-Lambda-Forge-Demo-HelloWorld function in the list of Lambda functions. </p> <p></p> <p>Once you've found your function, click on it to view its details. Proceed by selecting the Configuration tab, followed by Triggers to uncover the integration points.</p> <p></p> <p>In this tutorial, the generated URL is:</p> <p>https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world.</p> <p>By following the link provided, you should be greeted by a simple \"Hello World\" message in your web browser, indicating that your Lambda function is operational and accessible via the URL generated by API Gateway.</p> <pre><code>{\n  \"message\": \"Hello World!\"\n}\n</code></pre>"},{"location":"page2/#configuring-the-base-url-for-integration-tests","title":"Configuring the BASE URL for Integration Tests","text":"<p>With the Lambda function's URL at hand, we identify that the segment before <code>/hello_world</code> acts as the BASE URL. For the purposes of this tutorial, the BASE URL is <code>https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging</code>. This is the URL you need to copy.</p> <p>Proceed to incorporate this URL into your project's configuration by setting it as the value for <code>base_url</code> in your <code>cdk.json</code> file.</p> cdk.json<pre><code>...\n\"region\": \"us-east-2\",\n\"account\": \"\",\n\"name\": \"Lambda-Forge-Demo\",\n\"repo\": {\n    \"owner\": \"$GITHUB-USER\",\n    \"name\": \"$GITHUB-REPO\"\n},\n\"bucket\": \"\",\n\"coverage\": 80,\n\"base_url\": \"https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging\"\n...\n</code></pre>"},{"location":"page2/#pushing-updated-code-to-github","title":"Pushing Updated Code to GitHub","text":"<p>With the BASE URL configured for our integration tests, it's time to push the updated code to GitHub and aim for successful integration test outcomes across all deployment stages.</p> <p>Follow these steps to commit your changes and deploy them across the development, staging, and production branches:</p> <pre><code># Add changes to the staging area\ngit add .\n\n# Commit the changes with a descriptive message\ngit commit -m \"Configure BASE URL for integration tests\"\n\n# Push changes to the 'dev' branch.\ngit push origin dev\n\n# Switch to the 'staging' branch, merge changes from 'dev', and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Switch to the 'main' branch, merge changes from 'staging', and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>After executing these commands, all associated pipelines should be triggered once again.</p> <p></p> <p>Once the execution of all pipelines is complete, you should observe that all stages have successfully passed.</p> <p></p> <p></p> <p>Congratulations! \ud83c\udf89 You've successfully deployed your very first Lambda function across three distinct stages using Lambda Forge! \ud83d\ude80</p> <p>In this tutorial, the links are:</p> <ul> <li>Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/hello_world</li> <li>Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world</li> <li>Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/hello_world</li> </ul>"},{"location":"page3/","title":"Generating Docs","text":"<p>With our Lambda Function now deployed across three distinct stages, the next step involves creating documentation for our endpoints.</p>"},{"location":"page3/#setting-up-a-s3-bucket-for-documentation","title":"Setting Up a S3 Bucket for Documentation","text":"<p>Create an Amazon S3 bucket to serve as the primary storage for your documentation files. Follow these steps to create your S3 bucket:</p> <ol> <li>Access the AWS Management Console: Open the Amazon S3 console at https://console.aws.amazon.com/s3/.</li> <li>Create a New Bucket: Click on the \"Create bucket\" button. It's important to note that each bucket's name must be globally unique across all of Amazon S3.</li> <li>Set Bucket Name: Choose a unique and descriptive name for your bucket. This name will be crucial for accessing your documentation files. Remember, once a bucket name is set, it cannot be changed.</li> <li>Choose a Region: Select an AWS Region for your bucket. Choose the same region defined in your <code>cdk.json</code>.</li> <li>Configure Options: You may leave the default settings or configure additional options like versioning, logging, or add tags according to your needs.</li> <li>Review and Create: Before creating the bucket, review your settings. Once everything is confirmed, click \"Create bucket\".</li> </ol> <p>Once the bucket is created, update your <code>cdk.json</code> file with the bucket's name as shown below:</p> cdk.json<pre><code>...\n\"region\": \"us-east-2\",\n\"account\": \"\",\n\"name\": \"Lambda-Forge-Demo\",\n\"repo\": {\n    \"owner\": \"$GITHUB-USER\",\n    \"name\": \"$GITHUB-REPO\"\n},\n\"bucket\": \"$S3-BUCKET-NAME\",\n\"coverage\": 80,\n...\n</code></pre>"},{"location":"page3/#activating-docs","title":"Activating Docs","text":"<p>To activate documentation generation, navigate to the <code>deploy.py</code> file located at <code>infra/stages/deploy.py</code>. Modify the <code>enabled</code> parameter by setting it from <code>False</code> to <code>True</code> on line 13, as demonstrated below:</p> infra/stages/deploy.py<pre><code>import aws_cdk as cdk\nfrom constructs import Construct\n\nfrom infra.stacks.lambda_stack import LambdaStack\n\n\nclass DeployStage(cdk.Stage):\n    def __init__(self, scope: Construct, context, **kwargs):\n        super().__init__(scope, context.stage, **kwargs)\n\n        lambda_stack = LambdaStack(self, context)\n\n        lambda_stack.services.api_gateway.create_docs(enabled=True, authorizer=None)\n</code></pre> <p>By default, Lambda Forge does not include documentation for the Development stage.</p> <p>Since the project was initiated using the <code>--no-docs</code> flag, the generate docs step is absent from the post deployment phase in both the Staging and Production stacks.</p> <p>To activate the docs, update the relevant sections in your stack configurations as follows:</p> <p>In your Staging stack, enable generate docs by including it in the <code>post</code> array of the pipeline configuration. The updated section should look like this:</p> infra/stacks/staging_stack.py<pre><code>    # post\n    generate_docs = steps.generate_docs()\n    integration_tests = steps.run_integration_tests()\n\n    pipeline.add_stage(\n        DeployStage(self, context),\n        pre=[\n            unit_tests,\n            coverage,\n            validate_integration_tests,\n            validate_docs,\n        ],\n        post=[integration_tests, generate_docs], # Generate docs enabled\n    )\n</code></pre> <p>Similarly, for the Production stack, ensure that generate docs is enabled by adding it to the post section of your pipeline setup:</p> infra/stacks/prod_stack.py<pre><code>    # post\n    generate_docs = steps.generate_docs()\n\n    pipeline.add_stage(\n        DeployStage(self, context),\n        post=[generate_docs], # Generate docs enabled\n    )\n</code></pre> <p>At this point, we have all the necessary components to automatically generate our docs.</p> <p>To proceed, commit your changes and push them to GitHub using the following commands:</p> <pre><code>git add .\n\ngit commit -m \"Adding documentation support\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>Upon successful completion of the pipeline, the Swagger documentation for your endpoints can be accessed via the <code>/docs</code> path. This documentation provides comprehensive details about the available endpoints, including request formats, response structures, and query parameters.</p> <p>You can view the Swagger documentation at the following URLs for each environment:</p> <ul> <li>Staging Environment: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs</li> <li>Production Environment: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs</li> </ul> <p>This code snippet below demonstrates all the types of data you can expect to work with, including simple data types, lists, custom objects, optional fields, and literal types, offering a clear understanding of the input and output contracts for the API.</p> <pre><code>from dataclasses import dataclass\nfrom typing import List, Optional, Literal\n\n@dataclass\nclass Path:\n    id: str # If your endpoint requires a path parameter in the URL, document it here\n\n@dataclass\nclass Object:\n    a_string: str\n    an_int: int\n\n@dataclass\nclass Input:\n    a_string: str\n    an_int: int\n    a_boolean: bool\n    a_list: List[str]\n    an_object: Object\n    a_list_of_object: List[Object]\n    a_literal: Literal[\"a\", \"b\", \"c\"]\n    an_optional: Optional[str]\n\n@dataclass\nclass Output:\n    pass\n</code></pre> <p>You may have observed that our documentation endpoint is currently public. While in some scenarios, having public documentation is desirable, in others, it can pose a significant security risk. In the following section, we will explore methods to secure this and other endpoints, making them accessible only through an authorizer.</p>"},{"location":"page4/","title":"Private Endpoints","text":"<p>Given our current setup, all endpoints are accessible via the internet, which may or may not align with the intended use case of your application.</p> <p>Let's proceed to learn how to configure these endpoints to be private.</p> <p>First, let's begin by creating a new authorizer function with the following command:</p> <pre><code>forge authorizer docs --description \"Authorizer for Swagger Documentation\"\n</code></pre> <p>This command instructs the forge CLI tool to create a new authorizer function named docs.</p>"},{"location":"page4/#authorizer-structure","title":"Authorizer Structure","text":"<p>Authorizers, while closely resembling Lambda Functions in structure, fulfill a distinct role. Their primary function is to act as an intermediary layer that authenticates requests made to Lambda Functions, ensuring that only authorized requests are processed.</p> <p>Let's examine the structure of an authorizer more closely:</p> <pre><code>authorizers\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <ul> <li><code>authorizers/</code> This directory serves as the central hub for all authorizer functions, analogous to how the <code>functions/</code> directory houses Lambda functions. Each distinct authorizer is allocated its own subdirectory within this folder.</li> <li><code>docs/</code> The <code>docs</code> subdirectory is specifically designed for the authorizer related to the 'docs' endpoint. It includes everything needed for the authorizer to authenticate requests, manage configurations, and perform testing.</li> <li><code>__init__.py</code> Marks the directory as a Python package, enabling its modules to be imported elsewhere within the project.</li> <li><code>config.py</code> Contains the configuration settings for the authorizer, such as environmental variables and access control parameters. Keeping the configuration separate from the core logic is a best practice that enhances code maintainability and adaptability.</li> <li><code>main.py</code> Houses the main logic for the authorizer, detailing how incoming requests are verified. This file is where you define the procedures for credential validation and access authorization.</li> <li><code>unit.py</code> Focused on unit testing for the authorizer, these tests ensure that each part of the authorizer's code operates as expected independently, which is essential for early bug detection and reliable functionality.</li> <li><code>utils/</code> Provides utility functions that are used by the authorizers, offering common functionalities or resources that can be leveraged across various authorizers to facilitate development and maintenance.</li> </ul>"},{"location":"page4/#configuring-your-authorizer","title":"Configuring Your Authorizer","text":"<p>Similar to lambda functions in terms of setup, authorizers diverge in their application. Instead of establishing an endpoint on API Gateway, an authorizer is configured to control access to one or more endpoints.</p> authorizers/docs/config.py<pre><code>from infra.services import Services\n\nclass DocsAuthorizerConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"DocsAuthorizer\",\n            path=\"./authorizers/docs\",\n            description=\"Authorizer for Swagger Documentation\"\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"docs\", default=False)\n</code></pre> <p>Actually, Lambda Forge treats all lambda functions as private by default. That's why we had to use the <code>--public</code> flag in our initial forge command to make the function accessible without authentication. Without this flag, we would have been required to implement an authorizer for user authentication.</p>"},{"location":"page4/#implementing-the-authorizer","title":"Implementing The Authorizer","text":"<p>Forge automatically generates a basic implementation of an AWS Lambda authorizer. This example is intended solely for demonstration and learning purposes, and it is critical to devise a comprehensive and secure authentication mechanism suitable for your application's specific security needs. For demonstration, the authorizer checks a custom header for a specific secret value to decide on granting or denying access.</p> <p>Important Note: The example below employs a simple secret key for authorization, which is not recommended for production environments. It is crucial to replace this logic with a robust, secure authorization strategy before deploying your application.</p> authorizers/docs/main.py<pre><code>def lambda_handler(event, context):\n\n    # ATTENTION: The example provided below is strictly for demonstration purposes and should NOT be deployed in a production environment.\n    # It's crucial to develop and integrate your own robust authorization mechanism tailored to your application's security requirements.\n    # To utilize the example authorizer as a temporary placeholder, ensure to include the following header in your requests:\n\n    # Header:\n    # secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8\n\n    # Remember, security is paramount. This placeholder serves as a guide to help you understand the kind of information your custom authorizer should authenticate.\n    # Please replace it with your secure, proprietary logic before going live. Happy coding!\n\n    secret = event[\"headers\"].get(\"secret\")\n\n    SECRET = \"CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8\"\n    effect = \"allow\" if secret == SECRET else \"deny\"\n\n    policy = {\n        \"policyDocument\": {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Action\": \"execute-api:Invoke\",\n                    \"Effect\": effect,\n                    \"Resource\": \"*\"\n                }\n            ],\n        },\n    }\n    return policy\n</code></pre> <p>As illustrated in the code snippet above, the authorizer is designed to check for a specific header named <code>secret</code>, expecting it to contain the key <code>CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8</code>.</p> <p>This key serves as a simple form of authentication, granting or denying access based on its presence and accuracy in the request headers.</p> <p>The token mentioned is automatically generated by Forge, meaning the specific token you encounter during your implementation will differ from the example provided. Please be mindful of this distinction as you proceed.</p>"},{"location":"page4/#adding-authorizer-to-lambda-stack","title":"Adding Authorizer To Lambda Stack","text":"<p>Just like the functions, an authorizer needs to be initialized within the LambdaStack.</p> <p>Fortunately, Forge takes care of this automatically for you.</p> infra/stacks/lambda_stack.py<pre><code>from aws_cdk import Stack\nfrom constructs import Construct\nfrom infra.services import Services\nfrom lambda_forge import release\nfrom authorizers.docs.config import DocsAuthorizerConfig\nfrom functions.hello_world.config import HelloWorldConfig\n\n\n@release\nclass LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # Authorizers\n        DocsAuthorizerConfig(self.services)\n\n        # HelloWorld\n        HelloWorldConfig(self.services)\n</code></pre>"},{"location":"page4/#attaching-the-authorizer-to-the-docs-endpoints","title":"Attaching The Authorizer To The Docs Endpoints","text":"<p>Given that the resources developed in this tutorial serve as public demonstrations of the deployment process, we will not secure the previously created endpoint, allowing it to remain accessible to other readers. Instead, we'll introduce protection by setting up a new Swagger documentation under a separate endpoint.</p> <p>However, if you prefer, you're welcome to secure the original documentation endpoint instead of establishing a new one.</p> <p>To integrate the new authorizer or to update an existing docs endpoint to include authorization, navigate to your <code>infra/stages/deploy.py</code> file. Here, you will either add a new endpoint for the docs with authorization enabled or update the current one.</p> <p>The example below shows how to add a new endpoint <code>/docs/private</code> that is protected by the docs authorizer, while keeping the original docs endpoint public for demonstration purposes:</p> infra/stages/deploy.py<pre><code>import aws_cdk as cdk\nfrom constructs import Construct\n\nfrom infra.stacks.lambda_stack import LambdaStack\n\n\nclass DeployStage(cdk.Stage):\n    def __init__(self, scope: Construct, context, **kwargs):\n        super().__init__(scope, context.stage, **kwargs)\n\n        lambda_stack = LambdaStack(self, context)\n\n        # Keep the original docs endpoint public\n        lambda_stack.services.api_gateway.create_docs(enabled=True, authorizer=None)\n\n        # Add a new, private docs endpoint at /docs/private\n        lambda_stack.services.api_gateway.create_docs(enabled=True, authorizer=\"docs\", endpoint=\"/docs/private\")\n</code></pre> <p>Keep in mind that the authorizer you specify when enabling the docs endpoint must match the exact name assigned to the authorizer within the authorizer configuration class created previously.</p> authorizers/docs/config.py<pre><code>        function = services.aws_lambda.create_function(\n            name=\"DocsAuthorizer\",\n            path=\"./authorizers/docs\",\n            description=\"Authorizer to be used by swagger\"\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"docs\")\n</code></pre>"},{"location":"page4/#publish-your-changes-to-github","title":"Publish Your Changes to GitHub","text":"<p>With all components now properly configured, the next step is to publish your updates to GitHub. Execute the following commands to add, commit, and push your changes across the various branches, ensuring your repository reflects the latest state of your project:</p> <pre><code># Stage all changes for commit\ngit add .\n\n# Commit your changes with a descriptive message\ngit commit -m \"Implemented private documentation endpoint\"\n\n# Push the changes to the 'dev' branch\ngit push origin dev\n\n# Switch to the 'staging' branch, merge changes from 'dev', and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Switch to the 'main' branch, merge changes from 'staging', and push, completing the workflow\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>Once the deployment pipelines conclude, you'll have access to the newly secured documentation endpoint, protected by the authorizer.</p> <ul> <li>Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs/private</li> <li>Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs/private</li> </ul> <p>Attempting to access the above URLs directly will result in the following message:</p> <pre><code>{\n  \"Message\": \"User is not authorized to access this resource with an explicit deny\"\n}\n</code></pre> <p>This response confirms that the authorizer is operational, effectively restricting access to unauthorized requests.</p> <p>However, by including the appropriate secret in your request headers, you can gain access to the documentation.</p> <p>To view the documentation, copy the following cURL commands and paste them into your preferred API testing tool, such as Insomnia, Postman, or any other tool of your choice:</p> <p>For Staging:</p> Staging<pre><code>curl --request GET \\\n  --url https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs/private \\\n  --header 'secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8'\n</code></pre> <p>For Production:</p> Prod<pre><code>curl --request GET \\\n  --url https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs/private \\\n  --header 'secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8'\n</code></pre> <p>This process demonstrates the effectiveness of the authorizer in safeguarding your documentation, allowing only those with the correct secret to view it.</p>"},{"location":"page4/#default-authorizer","title":"Default Authorizer","text":"<p>As mentioned before, Lambda Forge treats all functions as private by default. Consequently, unless explicitly declared public, functions are presumed to necessitate an authorizer for access control.</p> <p>To streamline the process and eliminate the need to assign an authorizer to each Lambda function individually, you can designate a single authorizer as the default. This approach ensures that all non-public Lambda functions automatically inherit this default authorizer, simplifying the setup for access control.</p> <p>To establish a default authorizer, use the following command:</p> <pre><code>forge authorizer default --description \"Default Authorizer\" --default\n</code></pre> <p>This command configures a new authorizer named <code>default</code> within Forge, marking it as the default option. Consequently, any Lambda functions not explicitly associated with a different authorizer will automatically use this default authorizer for access control.</p> <p>Upon executing the command to create a default authorizer, a new directory structure is established under the authorizers folder, mirroring the organized approach taken for other components in the project. The structure now includes a dedicated folder for the <code>default</code> authorizer, alongside the <code>docs</code> authorizer.</p> <pre><code>authorizers\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 default\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <p>Let's examine the details of the <code>DefaultAuthorizerConfig</code> class recently created by Forge.</p> authorizers/default/config.py<pre><code>from infra.services import Services\n\nclass DefaultAuthorizerConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"DefaultAuthorizer\",\n            path=\"./authorizers/default\",\n            description=\"Default Authorizer\"\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"default\", default=True)\n</code></pre> <p>The <code>default=True</code> parameter on line 12 explicitly designates this authorizer as the default for all non-public Lambda functions, automatically applying it as their access control mechanism.</p> <p>Just like the docs authorizer, Forge has created a new secret key for the default authorizer. For this tutorial, the header expected in the request is:</p> <p><code>secret</code>: <code>Jmat02QiRNLTRVRWSUxBoljTxzpnHnzZcz0iFAXY4s1vgvO8m36q</code></p>"},{"location":"page4/#private-function","title":"Private Function","text":"<p>Now let's create a new private function.</p> <pre><code>Forge function private --method \"GET\" --description \"A private function\"\n</code></pre> <p>Upon creating a new private function using Forge with the specified command, the project's function structure is expanded to include this newly private function alongside the existing ones.</p> <pre><code>functions\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 hello_world\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 private\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre> <p>Let's examine the PrivateConfig class that Forge has generated for us.</p> functions/private/config.py<pre><code>from infra.services import Services\n\nclass PrivateConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Private\",\n            path=\"./functions/private\",\n            description=\"A private function\",\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/private\", function)\n</code></pre> <p>Within this configuration class, we are establishing a new endpoint to handle a GET request at the <code>/private</code> path. Importantly, there's no clear declaration of this function as public, nor is an authorizer explicitly defined for it. Nonetheless, due to the prior configuration of a default authorizer, this function will automatically fall under its protection.</p> <p>This approach underscores the utility and efficiency of setting a default authorizer safeguarding new functions by default, thereby enhancing security without necessitating manual authorizer configuration for each new endpoint.</p> <p>Let's make some adjustments to the response returned by this Lambda function:</p> functions/private/main.py<pre><code>def lambda_handler(event, context):\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"Hello From Private!\"})\n    }\n</code></pre> <p>Additionally, let's revise the unit tests and the integration tests to accurately represent the modifications we've implemented in our code.</p> functions/private/unit.py<pre><code>import json\nfrom .main import lambda_handler\n\ndef test_lambda_handler():\n\n    response = lambda_handler(None, None)\n\n    assert response[\"body\"] == json.dumps({\"message\": \"Hello From Private!\"})\n</code></pre> functions/private/integration.py<pre><code>import pytest\nimport requests\nfrom lambda_forge.constants import BASE_URL\n\n@pytest.mark.integration(method=\"GET\", endpoint=\"/private\")\ndef test_private_status_code_with_no_header_is_403():\n\n    response = requests.get(url=f\"{BASE_URL}/private\")\n\n    assert response.status_code == 403\n\n\n@pytest.mark.integration(method=\"GET\", endpoint=\"/private\")\ndef test_private_status_code_with_valid_header_is_200():\n\n    headers = {\n        \"secret\": \"Jmat02QiRNLTRVRWSUxBoljTxzpnHnzZcz0iFAXY4s1vgvO8m36q\"\n    }\n\n    response = requests.get(url=f\"{BASE_URL}/private\", headers=headers)\n\n    assert response.status_code == 200\n</code></pre> <p>Next, let's proceed to upload our updates to GitHub.</p> <pre><code># Add all changes to the staging area\ngit add .\n\n# Commit the staged changes with a clear message\ngit commit -m \"Implemented a private function with default authorizer\"\n\n# Push the committed changes to the 'dev' branch\ngit push origin dev\n\n# Transition to the 'staging' branch to integrate the latest developments\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, update the 'main' branch with the changes from 'staging' and push the update to complete the deployment process\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>Following the successful execution of our deployment pipelines, our private lambda function is now live:</p> <ul> <li>Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/private</li> <li>Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/private</li> </ul> <p>Attempting to access these URLs directly via your web browser will result in the following message, indicating unauthorized access:</p> <pre><code>{\n  \"Message\": \"User is not authorized to access this resource with an explicit deny\"\n}\n</code></pre> <p>However, by including the required secret in the header of your request, you can successfully retrieve the content. Here's how you can use curl to access the deployed Lambda function in both environments:</p> Staging<pre><code>curl --request GET \\\n  --url https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/private \\\n  --header 'secret: Jmat02QiRNLTRVRWSUxBoljTxzpnHnzZcz0iFAXY4s1vgvO8m36q'\n</code></pre> Prod<pre><code>curl --request GET \\\n  --url https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/private \\\n  --header 'secret: Jmat02QiRNLTRVRWSUxBoljTxzpnHnzZcz0iFAXY4s1vgvO8m36q'\n</code></pre> <p>This demonstrates the effectiveness of our authorizer in securing the private Lambda function, allowing access only to those with the correct secret header.</p>"},{"location":"page4/#using-specific-authorizers-for-endpoints","title":"Using Specific Authorizers for Endpoints","text":"<p>In case you need to secure an endpoint with a particular authorizer, you can achieve this by specifying the authorizer's name during its setup in the configuration class.</p> <p>Here\u2019s how you would configure a specific authorizer for an endpoint:</p> functions/private/config.py<pre><code>from infra.services import Services\n\nclass PrivateConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Private\",\n            path=\"./functions/private\",\n            description=\"A private function\",\n        )\n\n        # Specify the 'docs' authorizer for the '/private' endpoint\n        services.api_gateway.create_endpoint(\"GET\", \"/private\", function, authorizer=\"docs\")\n</code></pre> <p>By implementing this configuration, the <code>/private</code> function would be secured using the same authorizer as the <code>/docs</code> endpoints.</p> <p>Please note, this change is presented for illustrative purposes to demonstrate how to apply a non-default authorizer. Consequently, we will not commit this alteration and will continue to secure the <code>/private</code> endpoint with the default authorizer.</p>"},{"location":"page5/","title":"Building A Serverless CRUD Application With Dynamo DB","text":"<p>The primary goal of this tutorial is not to craft an elaborate application; instead, it focuses on showcasing the seamless integration of AWS resources within the Lambda Forge architecture. To highlight this, we will develop a straightforward CRUD application designed to capture and manage user-defined names and ages, each uniquely identified by a UUID. This approach not only simplifies the demonstration of the architecture's capabilities but also emphasizes the practical application of these technologies in a user-centric scenario.</p>"},{"location":"page5/#configuring-dynamodb-tables-for-each-deployment-stage","title":"Configuring DynamoDB Tables for Each Deployment Stage","text":"<p>To ensure our application can operate smoothly across different environments, we'll establish three separate DynamoDB tables, each tailored for a distinct deployment stage:</p> <ul> <li>Dev-Users</li> <li>Staging-Users</li> <li>Prod-Users</li> </ul> <p>Throughout this guide, we'll utilize <code>PK</code> as the Partition Key for our tables, optimizing data organization and access.</p> <p>Having acquired the ARNs for each stage-specific table, our next step involves integrating these ARNs into the <code>cdk.json</code> file. This crucial configuration enables our Cloud Development Kit (CDK) setup to correctly reference the DynamoDB tables according to the deployment stage.</p> <p>Here's how to update your <code>cdk.json</code> file to include the DynamoDB table ARNs for development, staging, and production environments:</p> cdk.json<pre><code>    \"dev\": {\n      \"arns\": {\n        \"users_table\": \"$DEV-USERS-TABLE-ARN\"\n      }\n    },\n    \"staging\": {\n      \"arns\": {\n        \"users_table\": \"$STAGING-USERS-TABLE-ARN\"\n      }\n    },\n    \"prod\": {\n      \"arns\": {\n        \"users_table\": \"$PROD-USERS-TABLE-ARN\"\n      }\n    }\n</code></pre>"},{"location":"page5/#incorporating-dynamodb-into-our-service-layer","title":"Incorporating DynamoDB Into Our Service Layer","text":"<p>The subsequent phase in enhancing our application involves integrating the DynamoDB service within our service layer, enabling direct communication with DynamoDB tables. To accomplish this, utilize the following command:</p> <p><code>forge service dynamo_db</code></p> <p>This command crafts a new service file named <code>dynamo_db.py</code> within the <code>infra/services</code> directory.</p> <pre><code>infra\n\u251c\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u2514\u2500\u2500 dynamo_db.py\n</code></pre> <p>Below is the updated structure of our Service class, now including the DynamoDB service, demonstrating the integration's completion:</p> infra/services/__init__.py<pre><code>from infra.services.dynamo_db import DynamoDB\nfrom infra.services.secrets_manager import SecretsManager\nfrom infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\n\n\nclass Services:\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n        self.dynamo_db = DynamoDB(scope, context.resources)\n</code></pre> <p>Here is the newly established DynamoDB class:</p> infra/services/dynamo_db.py<pre><code>from aws_cdk import aws_dynamodb as dynamo_db\nfrom aws_cdk import aws_iam as iam\n\n\nclass DynamoDB:\n    def __init__(self, scope, resources: dict) -&gt; None:\n\n        self.dynamo = dynamo_db.Table.from_table_arn(\n            scope,\n            \"Dynamo\",\n            resources[\"arns\"][\"dynamo_arn\"],\n        )\n\n    @staticmethod\n    def add_query_permission(function, table):\n        function.add_to_role_policy(\n            iam.PolicyStatement(\n                actions=[\"dynamodb:Query\"],\n                resources=[f\"{table.table_arn}/index/*\"],\n            )\n        )\n</code></pre> <p>In DynamoDB development, querying data is a fundamental operation. Notably, the DynamoDB class is equipped with a helper method designed to simplify the process of granting query permissions. Furthermore, we should refine the class variables to directly reference our Users table.</p> infra/services/dynamo_db.py<pre><code>from aws_cdk import aws_dynamodb as dynamo_db\nfrom aws_cdk import aws_iam as iam\n\n\nclass DynamoDB:\n    def __init__(self, scope, resources: dict) -&gt; None:\n\n        self.users_table = dynamo_db.Table.from_table_arn(\n            scope,\n            \"UsersTable\",\n            resources[\"arns\"][\"users_table\"],\n        )\n\n    @staticmethod\n    def add_query_permission(function, table):\n        function.add_to_role_policy(\n            iam.PolicyStatement(\n                actions=[\"dynamodb:Query\"],\n                resources=[f\"{table.table_arn}/index/*\"],\n            )\n        )\n</code></pre> <p>Ensure that the resource ARN precisely matches the name specified in your <code>cdk.json</code> file.</p>"},{"location":"page5/#building-the-create-feature","title":"Building the Create Feature","text":"<p>Next, we'll focus on constructing the \"Create\" functionality of our CRUD application. This feature is dedicated to inputting names and their corresponding ages into our DynamoDB tables. To initiate the creation of a Lambda function tailored for this operation, run the following command in the Forge CLI:</p> <pre><code>forge function create_user --method \"POST\" --description \"Create a user with name and age on Dynamo DB\" --belongs users --public\n</code></pre> <p>This command signals to Forge the need to generate a new Lambda function named create_user, which will handle POST requests. By applying the <code>--belongs</code> flag, we guide Forge to organize this function within the <code>users</code> directory, emphasizing its role as part of a suite of user-related functionalities.</p> <pre><code>functions/users\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <ul> <li><code>users/</code> This directory acts as the container for all Lambda functions related to users operations, organizing them under a common theme.</li> <li><code>create_user/</code> This subdirectory is dedicated to the function for creating users, equipped with all necessary files for its execution, configuration, and testing.</li> <li><code>utils/</code> A utility directory for shared functions or helpers that support the operations within the users functions, enhancing code reuse and maintainability.</li> </ul>"},{"location":"page5/#core-logic","title":"Core Logic","text":"<p>The Create User endpoint serves as the gateway for adding new users to our system. It processes incoming data from the request body, assigns a unique UUID to each user, and then stores this information in DynamoDB. Now, let's delve into the details of the function implementation.</p> functions/users/create_user/main.py<pre><code>import json\nimport uuid\nfrom dataclasses import dataclass\nimport os\nimport boto3\n\n\n@dataclass\nclass Input:\n    name: str\n    age: int\n\n\n@dataclass\nclass Output:\n    id: str\n\n\ndef lambda_handler(event, context):\n    # Retrieve the DynamoDB table name from environment variables.\n    USERS_TABLE = os.environ.get(\"USERS_TABLE\")\n\n    # Initialize a DynamoDB resource.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table.\n    users_table = dynamodb.Table(USERS_TABLE)\n\n    # Parse the request body to get user data.\n    body = json.loads(event[\"body\"])\n\n    # Generate a unique ID for the new user.\n    user_id = str(uuid.uuid4())\n\n    # Insert the new user into the DynamoDB table.\n    users_table.put_item(Item={\"PK\": user_id, \"name\": body[\"name\"], \"age\": body[\"age\"]})\n\n    # Return a successful response with the newly created user ID.\n    return {\"statusCode\": 200, \"body\": json.dumps({\"user_id\": user_id})}\n</code></pre>"},{"location":"page5/#configuration-class","title":"Configuration Class","text":"<p>Let's develop a configuration class to streamline the lambda function's access to necessary resources. This class will centralize the management of environment variables and resource configurations, thereby enhancing code maintainability and readability. It ensures that all external resources such as DynamoDB tables are easily configurable and securely accessed within the lambda function.</p> functions/users/create_user/config.py<pre><code>from infra.services import Services\n\n\nclass CreateUserConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"CreateUser\",\n            path=\"./functions/users\",\n            description=\"Create a user with name and age on Dynamo DB\",\n            directory=\"create_user\",\n            environment={\n                \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"POST\", \"/users\", function, public=True)\n\n        services.dynamo_db.users_table.grant_write_data(function)\n</code></pre>"},{"location":"page5/#unit-tests","title":"Unit Tests","text":"<p>Integrating AWS resources directly into our Lambda function introduces complexities when it comes to testing. Utilizing actual AWS services for unit testing is not optimal due to several reasons: it can incur unnecessary costs, lead to potential side effects on production data, and slow down testing due to reliance on internet connectivity and service response times. To address these challenges and ensure our tests are both efficient and isolated from real-world side effects, we'll simulate AWS resources using mock implementations. This approach allows us to control both the input and output, creating a more predictable and controlled testing environment.</p> <p>To facilitate this, we'll employ the moto library, which is specifically designed for mocking AWS services. This enables us to replicate AWS service responses without the need to interact with the actual services themselves.</p> <p>To get started with moto, install it using the following command:</p> <pre><code>pip install moto==4.7.1\n</code></pre> <p>Given that pytest is our chosen testing framework, it's worth highlighting how it utilizes fixtures to execute specific code segments before or after each test. Fixtures are a significant feature of pytest, enabling the setup and teardown of test environments or mock objects. This capability is particularly beneficial for our purposes, as it allows us to mock AWS resources effectively. By default, pytest automatically detects and loads fixtures defined in a file named <code>conftest.py</code>.</p> <p>Positioning our <code>conftest.py</code> file within the <code>functions/users</code> directory ensures that all unit tests within this scope can automatically access the defined fixtures. This strategic placement under the users folder allows every test in the directory to utilize the mocked AWS resources without additional configuration, streamlining the testing process for all tests related to the users functionality.</p> <p>Here's how the structure with the <code>conftest.py</code> file looks:</p> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <p>Below is the content of our fixture specifically designed to mock DynamoDB interactions.</p> functions/users/conftest.py<pre><code>import json\nimport os\nimport moto\nimport boto3\nimport pytest\n\n# Defines a pytest fixture with name users_table.\n@pytest.fixture\ndef users_table():\n    # Set an environment variable to use a fake table name within tests.\n    os.environ[\"USERS_TABLE_NAME\"] = \"FAKE-USERS-TABLE\"\n\n    # The `moto.mock_dynamodb` context manager simulates DynamoDB for the duration of the test.\n    with moto.mock_dynamodb():\n        db = boto3.client(\"dynamodb\")\n        db.create_table(\n            AttributeDefinitions=[\n                {\"AttributeName\": \"PK\", \"AttributeType\": \"S\"},\n            ],\n            TableName=\"FAKE-USERS-TABLE\",\n            KeySchema=[\n                {\"AttributeName\": \"PK\", \"KeyType\": \"HASH\"},\n            ],\n            BillingMode=\"PAY_PER_REQUEST\",\n        )\n\n        table = boto3.resource(\"dynamodb\").Table(\"FAKE-USERS-TABLE\")\n\n        # `yield` returns the table resource to the test function, ensuring cleanup after tests.\n        yield table\n</code></pre> <p>Having established this fixture, it is now readily available for use in our unit tests. Next, we will utilize this fixture to conduct tests on our create function, aiming to confirm its behavior under simulated conditions.</p> functions/users/create_user/unit.py<pre><code>import json\nfrom .main import lambda_handler\n\n\n# Test the create user function leveraging the users_table fixture from the conftest.py file automatically imported by pytest.\ndef test_lambda_handler(users_table):\n    # Simulate an event with a request body, mimicking a POST request payload containing a user's name and age.\n    event = {\"body\": json.dumps({\"name\": \"John Doe\", \"age\": 30})}\n\n    # Invoke the `lambda_handler` function with the simulated event and `None` for the context.\n    response = lambda_handler(event, None)\n\n    # Parse the JSON response body to work with the data as a Python dictionary.\n    response = json.loads(response[\"body\"])\n\n    # Retrieve the user item from the mocked DynamoDB table using the ID returned in the response.\n    # This action simulates the retrieval operation that would occur in a live DynamoDB instance.\n    user = users_table.get_item(Key={\"PK\": response[\"user_id\"]})[\"Item\"]\n\n    # Assert that the name and age in the DynamoDB item match the input values.\n    # These assertions confirm that the `lambda_handler` function correctly processes the input\n    # and stores the expected data in the DynamoDB table.\n    assert user[\"name\"] == \"John Doe\"\n    assert user[\"age\"] == 30\n</code></pre> <p>By running the command <code>pytest functions/users -k unit</code>, we initiate the execution of only the unit tests located within the <code>functions/users</code> directory.</p> <pre><code>============================ test session starts ==============================\n\nplatform darwin -- Python 3.10.4, pytest-8.1.1, pluggy-1.4.0\nconfigfile: pytest.ini\ncollected 2 items / 1 deselected / 1 selected\n\nfunctions/users/create_user/unit.py .                                    [100%]\n\n=========================== 1 passed, 1 deselected in 2.45s ===================\n</code></pre> <p>As evidenced, our unit test has successfully passed.</p>"},{"location":"page5/#integration-tests","title":"Integration Tests","text":"<p>Unlike unit tests, which are isolated and focused on individual components, integration tests play a critical role in our testing strategy by utilizing actual resources. This approach allows us to verify the effective interaction between various parts of our application, particularly the endpoints and the resources they rely on. Integration testing uncovers issues that may not be apparent during unit testing, such as:</p> <ul> <li>Incorrect or insufficient permissions that prevent functions from accessing databases</li> <li>Configuration errors that could lead to service disruptions</li> <li>Network issues affecting the communication between services</li> <li>Potential security vulnerabilities in the integration points</li> </ul> <p>Conducting a simple POST request and expecting a 200 response code is a basic, yet critical, integration test. It serves as a primary indicator of the endpoint's operability. In this tutorial, we'll focus solely on this basic test case to keep the content accessible and prevent information overload.</p> <p>However, it's worth noting that integration testing can encompass a wide array of scenarios, such as verifying the successful insertion of data into a DynamoDB table using Boto3, among others.</p> functions/users/create_user/integration.py<pre><code>import pytest\nimport requests\nfrom lambda_forge.constants import BASE_URL\n\n@pytest.mark.integration(method=\"POST\", endpoint=\"/users\")\ndef test_create_user_status_code_is_200():\n\n    response = requests.post(url=f\"{BASE_URL}/users\", json={\"name\": \"John Doe\", \"age\": 30})\n\n    assert response.status_code == 200\n</code></pre> <p>The advantage of employing a multi-stage environment is especially pronounced in the context of integration testing. Since our base url is pointing to the staging environment, we ensure that our testing activities do not adversely affect the production environment.</p>"},{"location":"page5/#disclaimer","title":"Disclaimer","text":"<p>To maintain our focus on illustrating the setup and interactions with AWS resources and the Lambda Forge architecture, we will intentionally skip detailed coverage of subsequent unit and integration tests.This approach is also informed by the fact that these tests will not significantly differ from those we've outlined for the <code>create user</code> endpoint. Including them would introduce redundancy and potentially overwhelm the reader with excessive detail. Our primary goal is to ensure clarity and conciseness while providing a comprehensive understanding of the key concepts.</p> <p>However, rest assured that all the code developed in this tutorial, along with the tests, will be made available on GitHub for future reference and deeper exploration. This way, we aim to strike a balance between thoroughness and accessibility, ensuring that the tutorial remains engaging and informative without causing reader fatigue.</p>"},{"location":"page5/#building-the-read-feature","title":"Building the Read Feature","text":"<p>We're now set to construct the read feature, enabling the retrieval of user details using their ID.</p> <p>To facilitate this, we'll utilize the following command:</p> <pre><code>forge function get_user --method \"GET\" --description \"Retrieve user information by ID\" --belongs users --endpoint \"/users/{user_id}\" --public\n</code></pre> <p>The <code>--endpoint \"/users/{user_id}\"</code> parameter sets up a specific URL path for accessing this function. This path includes a dynamic segment {user_id} that gets replaced by the actual ID of the user we're trying to retrieve information about when the function is called.</p> <p>By running this command, we add a new layer to our application that specifically handles fetching user details in an organized, accessible manner.</p> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 get_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"page5/#core-logic_1","title":"Core Logic","text":"functions/users/get_user/main.py<pre><code>import json\nimport os\nimport boto3\nfrom dataclasses import dataclass\n\n@dataclass\nclass Path:\n    user_id: str\n\n@dataclass\nclass Input:\n    pass\n\n@dataclass\nclass Output:\n    name: str\n    age: int\n\ndef lambda_handler(event, context):\n    # Retrieve the name of the DynamoDB table from environment variables.\n    USERS_TABLE_NAME = os.environ.get(\"USERS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource using boto3.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the specific DynamoDB table by name.\n    users_table = dynamodb.Table(USERS_TABLE_NAME)\n\n    # Extract the user ID from the pathParameters provided in the event object.\n    user_id = event[\"pathParameters\"].get(\"user_id\")\n\n    # Retrieve the user item from the DynamoDB table using the extracted ID.\n    user = users_table.get_item(Key={\"PK\": user_id}).get(\"Item\")\n\n    # Reformat the user item into the desired output structure.\n    user = {\"name\": user[\"name\"], \"age\": user[\"age\"]}\n\n    # Return the user data with a 200 status code, ensuring the body is properly JSON-encoded.\n    return {\"statusCode\": 200, \"body\": json.dumps(user, default=str)}\n</code></pre>"},{"location":"page5/#configuration-class_1","title":"Configuration Class","text":"functions/users/get_user/config.py<pre><code>from infra.services import Services\n\n\nclass GetUserConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"GetUser\",\n            path=\"./functions/users\",\n            description=\"Retrieve user information by ID\",\n            directory=\"get_user\",\n            environment={\n                \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\n            \"GET\", \"/users/{user_id}\", function, public=True\n        )\n\n        services.dynamo_db.users_table.grant_read_data(function)\n</code></pre>"},{"location":"page5/#building-the-update-feature","title":"Building the Update Feature","text":"<pre><code>forge function update_user --method \"PUT\" --description \"Update an user by ID\" --belongs users --endpoint \"/users/{user_id}\" --public\n</code></pre> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 get_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 update_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"page5/#core-logic_2","title":"Core Logic","text":"functions/users/update_user/main.py<pre><code>import json\nfrom dataclasses import dataclass\nimport os\nimport boto3\n\n\n@dataclass\nclass Path:\n    user_id: str\n\n\n@dataclass\nclass Input:\n    name: str\n    age: int\n\n\n@dataclass\nclass Output:\n    message: str\n\ndef lambda_handler(event, context):\n    # Retrieve the DynamoDB table name from environment variables set in the Lambda configuration\n    USERS_TABLE_NAME = os.environ.get(\"USERS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource using boto3, AWS's SDK for Python\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table using the retrieved table name\n    users_table = dynamodb.Table(USERS_TABLE_NAME)\n\n    # Extract the user ID from the pathParameters of the event object passed to the Lambda\n    user_id = event[\"pathParameters\"].get(\"user_id\")\n\n    # Parse the JSON body from the event object to get the user data\n    body = json.loads(event[\"body\"])\n\n    # Update the specified user item in the DynamoDB table with the provided name and age\n    users_table.put_item(Item={\"PK\": user_id, \"name\": body[\"name\"], \"age\": body[\"age\"]})\n\n    # Return a response indicating successful user update, with a 200 HTTP status code\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"User updated\"}, default=str),\n    }\n</code></pre>"},{"location":"page5/#configuration-class_2","title":"Configuration Class","text":"functions/users/update_user/config.py<pre><code>from infra.services import Services\n\nclass UpdateUserConfig:\n  def __init__(self, services: Services) -&gt; None:\n\n    function = services.aws_lambda.create_function(\n        name=\"UpdateUser\",\n        path=\"./functions/users\",\n        description=\"Update an User\",\n        directory=\"update_user\",\n        environment={\n            \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n        },\n    )\n\n    services.api_gateway.create_endpoint(\n        \"PUT\", \"/users/{user_id}\", function, public=True\n    )\n\n    services.dynamo_db.users_table.grant_write_data(function)\n</code></pre>"},{"location":"page5/#building-the-delete-feature","title":"Building the Delete Feature","text":"<pre><code>forge function delete_user --method \"DELETE\" --description \"Delete an user by ID\" --belongs users --endpoint \"/users/{user_id}\" --public\n</code></pre> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 delete_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 get_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 update_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"page5/#core-logic_3","title":"Core Logic","text":"functions/users/delete_user/main.py<pre><code>import json\nfrom dataclasses import dataclass\nimport os\nimport boto3\n\n@dataclass\nclass Path:\n    user_id: str\n\n@dataclass\nclass Input:\n    pass\n\n@dataclass\nclass Output:\n    message: str\n\n\ndef lambda_handler(event, context):\n    # Fetch the name of the DynamoDB table from the environment variables.\n    USERS_TABLE_NAME = os.environ.get(\"USERS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource using the boto3 library.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table by its name.\n    users_table = dynamodb.Table(USERS_TABLE_NAME)\n\n    # Extract the user ID from the path parameters in the event object.\n    user_id = event[\"pathParameters\"].get(\"user_id\")\n\n    # Delete the item with the specified user ID from the DynamoDB table.\n    users_table.delete_item(Key={\"PK\": user_id})\n\n    # Return a response indicating that the user has been successfully deleted, with a 200 HTTP status code.\n    return {\"statusCode\": 200, \"body\": json.dumps({\"message\": \"User deleted\"})}\n</code></pre>"},{"location":"page5/#configuration-class_3","title":"Configuration Class","text":"functions/users/delete_user/config.py<pre><code>from infra.services import Services\n\n\nclass DeleteUserConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"DeleteUser\",\n            path=\"./functions/users\",\n            description=\"Delete an User\",\n            directory=\"delete_user\",\n            environment={\n                \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\n            \"DELETE\", \"/users/{user_id}\", function, public=True\n        )\n\n        services.dynamo_db.users_table.grant_write_data(function)\n</code></pre>"}]}