{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"examples/page1/","title":"Introduction","text":"<p>In this section, we embark on crafting a series of projects, serving as practical examples to illustrate the development flow with Lambda Forge. These projects will be introduced incrementally, coexisting within a single repository.</p> <p>This incremental approach ensures that once a service is introduced in a previous project, it won't be reinitialized in subsequent ones. Instead, we will seamlessly build upon the foundation laid by preceding projects, adding new components and functionalities as we progress.</p> <p>With this understanding, let's initiate a new project and dive into coding!</p> <pre><code>forge project lambda-forge-examples --repo-owner \"$GITHUB-OWNER\" --repo-name \"$GITHUB-REPO\" --bucket \"$S3-BUCKET\" --public-docs\n</code></pre> <p>Docs: https://mdiamz20p2.execute-api.us-east-2.amazonaws.com/prod/docs.</p> <p>Source code: https://github.com/GuiPimenta-Dev/lambda-forge-examples</p>"},{"location":"examples/page2/","title":"Building A Serverless CRUD Application With Dynamo DB","text":"<p>In this section, we will develop a straightforward CRUD application designed to capture and manage user-defined names and ages, each uniquely identified by a UUID. This approach not only simplifies the demonstration of the architecture's capabilities but also emphasizes the practical application of these technologies in a user-centric scenario.</p>"},{"location":"examples/page2/#configuring-dynamodb-tables-for-each-deployment-stage","title":"Configuring DynamoDB Tables for Each Deployment Stage","text":"<p>To ensure our application can operate smoothly across different environments, we'll create three separate DynamoDB tables on AWS DynamoDB console, each tailored for a distinct deployment stage:</p> <ul> <li>Dev-Users</li> <li>Staging-Users</li> <li>Prod-Users</li> </ul> <p>Throughout this guide, we'll utilize <code>PK</code> as the Partition Key for our tables, optimizing data organization and access.</p> <p>Having acquired the ARNs for each stage-specific table, our next step involves integrating these ARNs into the <code>cdk.json</code> file. This crucial configuration enables our Cloud Development Kit (CDK) setup to correctly reference the DynamoDB tables according to the deployment stage.</p> <p>Here's how to update your <code>cdk.json</code> file to include the DynamoDB table ARNs for development, staging, and production environments:</p> cdk.json<pre><code>    \"dev\": {\n      \"arns\": {\n        \"users_table\": \"$DEV-USERS-TABLE-ARN\"\n      }\n    },\n    \"staging\": {\n      \"arns\": {\n        \"users_table\": \"$STAGING-USERS-TABLE-ARN\"\n      }\n    },\n    \"prod\": {\n      \"arns\": {\n        \"users_table\": \"$PROD-USERS-TABLE-ARN\"\n      }\n    }\n</code></pre>"},{"location":"examples/page2/#incorporating-dynamodb-into-our-service-layer","title":"Incorporating DynamoDB Into Our Service Layer","text":"<p>The subsequent phase in enhancing our application involves integrating the DynamoDB service within our service layer, enabling direct communication with DynamoDB tables. To accomplish this, utilize the following command:</p> <p><code>forge service dynamo_db</code></p> <p>This command crafts a new service file named <code>dynamo_db.py</code> within the <code>infra/services</code> directory.</p> <pre><code>infra\n\u251c\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u2514\u2500\u2500 dynamo_db.py\n</code></pre> <p>Below is the updated structure of our Service class, now including the DynamoDB service, demonstrating the integration's completion:</p> infra/services/__init__.py<pre><code>from infra.services.dynamo_db import DynamoDB\nfrom infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\n\n\nclass Services:\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n        self.dynamo_db = DynamoDB(scope, context.resources)\n</code></pre> <p>Here is the newly established DynamoDB class:</p> infra/services/dynamo_db.py<pre><code>from aws_cdk import aws_dynamodb as dynamo_db\nfrom aws_cdk import aws_iam as iam\n\n\nclass DynamoDB:\n    def __init__(self, scope, resources: dict) -&gt; None:\n\n        self.dynamo = dynamo_db.Table.from_table_arn(\n            scope,\n            \"Dynamo\",\n            resources[\"arns\"][\"dynamo_arn\"],\n        )\n\n    @staticmethod\n    def add_query_permission(function, table):\n        function.add_to_role_policy(\n            iam.PolicyStatement(\n                actions=[\"dynamodb:Query\"],\n                resources=[f\"{table.table_arn}/index/*\"],\n            )\n        )\n</code></pre> <p>In DynamoDB development, querying data is a fundamental operation. Notably, the DynamoDB class is equipped with a helper method designed to simplify the process of granting query permissions. Furthermore, we should refine the class variables to directly reference our Users table.</p> infra/services/dynamo_db.py<pre><code>class DynamoDB:\n    def __init__(self, scope, resources: dict) -&gt; None:\n\n        self.users_table = dynamo_db.Table.from_table_arn(\n            scope,\n            \"UsersTable\",\n            resources[\"arns\"][\"users_table\"],\n        )\n</code></pre> <p>Ensure that the resource ARN precisely matches the name specified in your <code>cdk.json</code> file.</p>"},{"location":"examples/page2/#building-the-create-feature","title":"Building the Create Feature","text":"<p>Next, we'll focus on constructing the \"Create\" functionality of our CRUD application. This feature is dedicated to inputting names and their corresponding ages into our DynamoDB tables. To initiate the creation of a Lambda function tailored for this operation, run the following command in the Forge CLI:</p> <pre><code>forge function create_user --method \"POST\" --description \"Create a user with name and age on Dynamo DB\" --belongs users --public\n</code></pre> <p>This command signals to Forge the need to generate a new Lambda function named create_user, which will handle POST requests. By applying the <code>--belongs</code> flag, we guide Forge to organize this function within the <code>users</code> directory, emphasizing its role as part of a suite of user-related functionalities.</p> <pre><code>functions/users\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <ul> <li><code>users/</code> This directory acts as the container for all Lambda functions related to users operations, organizing them under a common theme.</li> <li><code>create_user/</code> This subdirectory is dedicated to the function for creating users, equipped with all necessary files for its execution, configuration, and testing.</li> <li><code>utils/</code> A utility directory for shared functions or helpers that support the operations within the users functions, enhancing code reuse and maintainability.</li> </ul>"},{"location":"examples/page2/#core-logic","title":"Core Logic","text":"<p>The Create User endpoint serves as the gateway for adding new users to our system. It processes incoming data from the request body, assigns a unique UUID to each user, and then stores this information in DynamoDB. Now, let's delve into the details of the function implementation.</p> functions/users/create_user/main.py<pre><code>import json\nimport uuid\nfrom dataclasses import dataclass\nimport os\nimport boto3\n\n\n@dataclass\nclass Input:\n    name: str\n    age: int\n\n\n@dataclass\nclass Output:\n    id: str\n\n\ndef lambda_handler(event, context):\n    # Retrieve the DynamoDB table name from environment variables.\n    USERS_TABLE = os.environ.get(\"USERS_TABLE\")\n\n    # Initialize a DynamoDB resource.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table.\n    users_table = dynamodb.Table(USERS_TABLE)\n\n    # Parse the request body to get user data.\n    body = json.loads(event[\"body\"])\n\n    # Generate a unique ID for the new user.\n    user_id = str(uuid.uuid4())\n\n    # Insert the new user into the DynamoDB table.\n    users_table.put_item(Item={\"PK\": user_id, \"name\": body[\"name\"], \"age\": body[\"age\"]})\n\n    # Return a successful response with the newly created user ID.\n    return {\"statusCode\": 200, \"body\": json.dumps({\"user_id\": user_id})}\n</code></pre>"},{"location":"examples/page2/#configuration-class","title":"Configuration Class","text":"<p>Let's develop a configuration class to streamline the lambda function's access to necessary resources. This class will centralize the management of environment variables and resource configurations, thereby enhancing code maintainability and readability. It ensures that all external resources such as DynamoDB tables are easily configurable and securely accessed within the lambda function.</p> functions/users/create_user/config.py<pre><code>from infra.services import Services\n\n\nclass CreateUserConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"CreateUser\",\n            path=\"./functions/users\",\n            description=\"Create a user with name and age on Dynamo DB\",\n            directory=\"create_user\",\n            environment={\n                \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\"POST\", \"/users\", function, public=True)\n\n        services.dynamo_db.users_table.grant_write_data(function)\n</code></pre>"},{"location":"examples/page2/#unit-tests","title":"Unit Tests","text":"<p>Integrating AWS resources directly into our Lambda function introduces complexities when it comes to testing. Utilizing actual AWS services for unit testing is not optimal due to several reasons: it can incur unnecessary costs, lead to potential side effects on production data, and slow down testing due to reliance on internet connectivity and service response times. To address these challenges and ensure our tests are both efficient and isolated from real-world side effects, we'll simulate AWS resources using mock implementations. This approach allows us to control both the input and output, creating a more predictable and controlled testing environment.</p> <p>To facilitate this, we'll employ the moto library, which is specifically designed for mocking AWS services. This enables us to replicate AWS service responses without the need to interact with the actual services themselves.</p> <p>To get started with moto, install it using the following command:</p> <pre><code>pip install moto==4.7.1\n</code></pre> <p>Given that pytest is our chosen testing framework, it's worth highlighting how it utilizes fixtures to execute specific code segments before or after each test. Fixtures are a significant feature of pytest, enabling the setup and teardown of test environments or mock objects. This capability is particularly beneficial for our purposes, as it allows us to mock AWS resources effectively. By default, pytest automatically detects and loads fixtures defined in a file named <code>conftest.py</code>.</p> <p>Positioning our <code>conftest.py</code> file within the <code>functions/users</code> directory ensures that all unit tests within this scope can automatically access the defined fixtures. This strategic placement under the users folder allows every test in the directory to utilize the mocked AWS resources without additional configuration, streamlining the testing process for all tests related to the users functionality.</p> <p>Here's how the structure with the <code>conftest.py</code> file looks:</p> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <p>Below is the content of our fixture specifically designed to mock DynamoDB interactions.</p> functions/users/conftest.py<pre><code>import json\nimport os\nimport moto\nimport boto3\nimport pytest\n\n# Defines a pytest fixture with name users_table.\n@pytest.fixture\ndef users_table():\n    # Set an environment variable to use a fake table name within tests.\n    os.environ[\"USERS_TABLE_NAME\"] = \"FAKE-USERS-TABLE\"\n\n    # The `moto.mock_dynamodb` context manager simulates DynamoDB for the duration of the test.\n    with moto.mock_dynamodb():\n        db = boto3.client(\"dynamodb\")\n        db.create_table(\n            AttributeDefinitions=[\n                {\"AttributeName\": \"PK\", \"AttributeType\": \"S\"},\n            ],\n            TableName=\"FAKE-USERS-TABLE\",\n            KeySchema=[\n                {\"AttributeName\": \"PK\", \"KeyType\": \"HASH\"},\n            ],\n            BillingMode=\"PAY_PER_REQUEST\",\n        )\n\n        table = boto3.resource(\"dynamodb\").Table(\"FAKE-USERS-TABLE\")\n\n        # `yield` returns the table resource to the test function, ensuring cleanup after tests.\n        yield table\n</code></pre> <p>Having established this fixture, it is now readily available for use in our unit tests. Next, we will utilize this fixture to conduct tests on our create function, aiming to confirm its behavior under simulated conditions.</p> functions/users/create_user/unit.py<pre><code>import json\nfrom .main import lambda_handler\n\n\n# Test the create user function leveraging the users_table fixture from the conftest.py file automatically imported by pytest.\ndef test_lambda_handler(users_table):\n    # Simulate an event with a request body, mimicking a POST request payload containing a user's name and age.\n    event = {\"body\": json.dumps({\"name\": \"John Doe\", \"age\": 30})}\n\n    # Invoke the `lambda_handler` function with the simulated event and `None` for the context.\n    response = lambda_handler(event, None)\n\n    # Parse the JSON response body to work with the data as a Python dictionary.\n    response = json.loads(response[\"body\"])\n\n    # Retrieve the user item from the mocked DynamoDB table using the ID returned in the response.\n    # This action simulates the retrieval operation that would occur in a live DynamoDB instance.\n    user = users_table.get_item(Key={\"PK\": response[\"user_id\"]})[\"Item\"]\n\n    # Assert that the name and age in the DynamoDB item match the input values.\n    # These assertions confirm that the `lambda_handler` function correctly processes the input\n    # and stores the expected data in the DynamoDB table.\n    assert user[\"name\"] == \"John Doe\"\n    assert user[\"age\"] == 30\n</code></pre> <p>By running the command <code>pytest functions/users -k unit</code>, we initiate the execution of only the unit tests located within the <code>functions/users</code> directory.</p> <pre><code>============================ test session starts ==============================\n\nplatform darwin -- Python 3.10.4, pytest-8.1.1, pluggy-1.4.0\nconfigfile: pytest.ini\ncollected 2 items / 1 deselected / 1 selected\n\nfunctions/users/create_user/unit.py .                                    [100%]\n\n=========================== 1 passed, 1 deselected in 2.45s ===================\n</code></pre> <p>As evidenced, our unit test has successfully passed.</p>"},{"location":"examples/page2/#integration-tests","title":"Integration Tests","text":"<p>Unlike unit tests, which are isolated and focused on individual components, integration tests play a critical role in our testing strategy by utilizing actual resources. This approach allows us to verify the effective interaction between various parts of our application, particularly the endpoints and the resources they rely on. Integration testing uncovers issues that may not be apparent during unit testing, such as:</p> <ul> <li>Incorrect or insufficient permissions that prevent functions from accessing databases</li> <li>Configuration errors that could lead to service disruptions</li> <li>Network issues affecting the communication between services</li> <li>Potential security vulnerabilities in the integration points</li> </ul> <p>Conducting a simple POST request and expecting a 200 response code is a basic, yet critical, integration test. It serves as a primary indicator of the endpoint's operability. In this tutorial, we'll focus solely on this basic test case to keep the content accessible and prevent information overload.</p> <p>However, it's worth noting that integration testing can encompass a wide array of scenarios, such as verifying the successful insertion of data into a DynamoDB table using Boto3, among others.</p> functions/users/create_user/integration.py<pre><code>import pytest\nimport requests\nfrom lambda_forge.constants import BASE_URL\n\n@pytest.mark.integration(method=\"POST\", endpoint=\"/users\")\ndef test_create_user_status_code_is_200():\n\n    response = requests.post(url=f\"{BASE_URL}/users\", json={\"name\": \"John Doe\", \"age\": 30})\n\n    assert response.status_code == 200\n</code></pre> <p>The advantage of employing a multi-stage environment is especially pronounced in the context of integration testing. Since our base url is pointing to the staging environment, we ensure that our testing activities do not adversely affect the production environment.</p>"},{"location":"examples/page2/#disclaimer","title":"Disclaimer","text":"<p>To maintain our focus on illustrating the setup and interactions with AWS resources and the Lambda Forge architecture, we will intentionally skip detailed coverage of subsequent unit and integration tests.</p> <p>This approach is also informed by the fact that these tests will not significantly differ from those we've outlined for the <code>create user</code> endpoint. Including them would introduce redundancy and potentially overwhelm the reader with excessive detail. Our primary goal is to ensure clarity and conciseness while providing a comprehensive understanding of the key concepts.</p> <p>However, rest assured that all the code developed in this tutorial, along with the tests, is available on GitHub for future reference and deeper exploration. This way, we aim to strike a balance between thoroughness and accessibility, ensuring that the tutorial remains engaging and informative without causing reader fatigue.</p>"},{"location":"examples/page2/#building-the-read-feature","title":"Building the Read Feature","text":"<p>We're now set to construct the read feature, enabling the retrieval of user details using their ID.</p> <p>To facilitate this, we'll utilize the following command:</p> <pre><code>forge function get_user --method \"GET\" --description \"Retrieve user information by ID\" --belongs users --endpoint \"/users/{user_id}\" --public\n</code></pre> <p>The <code>--endpoint \"/users/{user_id}\"</code> parameter sets up a specific URL path for accessing this function. This path includes a dynamic segment {user_id} that gets replaced by the actual ID of the user we're trying to retrieve information about when the function is called.</p> <p>By running this command, we add a new layer to our application that specifically handles fetching user details in an organized, accessible manner.</p> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 get_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"examples/page2/#core-logic_1","title":"Core Logic","text":"<p>This segment of our application demonstrates the retrieval of user information from a DynamoDB table through an AWS Lambda function. It highlights how to parse API gateway events, interact with DynamoDB, and structure responses for efficient data delivery.</p> functions/users/get_user/main.py<pre><code>import json\nimport os\nimport boto3\nfrom dataclasses import dataclass\n\n@dataclass\nclass Path:\n    user_id: str\n\n@dataclass\nclass Input:\n    pass\n\n@dataclass\nclass Output:\n    name: str\n    age: int\n\ndef lambda_handler(event, context):\n    # Retrieve the name of the DynamoDB table from environment variables.\n    USERS_TABLE_NAME = os.environ.get(\"USERS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource using boto3.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the specific DynamoDB table by name.\n    users_table = dynamodb.Table(USERS_TABLE_NAME)\n\n    # Extract the user ID from the pathParameters provided in the event object.\n    user_id = event[\"pathParameters\"].get(\"user_id\")\n\n    # Retrieve the user item from the DynamoDB table using the extracted ID.\n    user = users_table.get_item(Key={\"PK\": user_id}).get(\"Item\")\n\n    # Reformat the user item into the desired output structure.\n    user = {\"name\": user[\"name\"], \"age\": user[\"age\"]}\n\n    # Return the user data with a 200 status code, ensuring the body is properly JSON-encoded.\n    return {\"statusCode\": 200, \"body\": json.dumps(user, default=str)}\n</code></pre>"},{"location":"examples/page2/#configuration-class_1","title":"Configuration Class","text":"<p>The config class below outlines the configuration necessary for establishing the GetUser function within AWS, illustrating the seamless integration of AWS Lambda and API Gateway to expose a user data retrieval endpoint.</p> functions/users/get_user/config.py<pre><code>from infra.services import Services\n\n\nclass GetUserConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"GetUser\",\n            path=\"./functions/users\",\n            description=\"Retrieve user information by ID\",\n            directory=\"get_user\",\n            environment={\n                \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\n            \"GET\", \"/users/{user_id}\", function, public=True\n        )\n\n        services.dynamo_db.users_table.grant_read_data(function)\n</code></pre>"},{"location":"examples/page2/#building-the-update-feature","title":"Building the Update Feature","text":"<p>Let's utilize Forge once again to swiftly establish a tailored structure, setting the stage for our Update User functionality.</p> <pre><code>forge function update_user --method \"PUT\" --description \"Update an user by ID\" --belongs users --endpoint \"/users/{user_id}\" --public\n</code></pre> <p>As expected, after using the forge command to generate the <code>update_user</code> function, a predefined directory structure is created.</p> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 get_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 update_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"examples/page2/#core-logic_2","title":"Core Logic","text":"<p>Below is the implementation for updating a user, allowing changes to either the name or age.</p> functions/users/update_user/main.py<pre><code>import json\nfrom dataclasses import dataclass\nimport os\nimport boto3\n\n\n@dataclass\nclass Path:\n    user_id: str\n\n\n@dataclass\nclass Input:\n    name: str\n    age: int\n\n\n@dataclass\nclass Output:\n    message: str\n\ndef lambda_handler(event, context):\n    # Retrieve the DynamoDB table name from environment variables set in the Lambda configuration\n    USERS_TABLE_NAME = os.environ.get(\"USERS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource using boto3, AWS's SDK for Python\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table using the retrieved table name\n    users_table = dynamodb.Table(USERS_TABLE_NAME)\n\n    # Extract the user ID from the pathParameters of the event object passed to the Lambda\n    user_id = event[\"pathParameters\"].get(\"user_id\")\n\n    # Parse the JSON body from the event object to get the user data\n    body = json.loads(event[\"body\"])\n\n    # Update the specified user item in the DynamoDB table with the provided name and age\n    users_table.put_item(Item={\"PK\": user_id, \"name\": body[\"name\"], \"age\": body[\"age\"]})\n\n    # Return a response indicating successful user update, with a 200 HTTP status code\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"User updated\"}, default=str),\n    }\n</code></pre>"},{"location":"examples/page2/#configuration-class_2","title":"Configuration Class","text":"<p>Here's the configuration needed for the <code>update user</code> function to properly engage with the essential AWS services.</p> functions/users/update_user/config.py<pre><code>from infra.services import Services\n\nclass UpdateUserConfig:\n  def __init__(self, services: Services) -&gt; None:\n\n    function = services.aws_lambda.create_function(\n        name=\"UpdateUser\",\n        path=\"./functions/users\",\n        description=\"Update an User\",\n        directory=\"update_user\",\n        environment={\n            \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n        },\n    )\n\n    services.api_gateway.create_endpoint(\n        \"PUT\", \"/users/{user_id}\", function, public=True\n    )\n\n    services.dynamo_db.users_table.grant_write_data(function)\n</code></pre>"},{"location":"examples/page2/#building-the-delete-feature","title":"Building the Delete Feature","text":"<p>Now, to complete our CRUD application, let's proceed with constructing the Delete User endpoint.</p> <pre><code>forge function delete_user --method \"DELETE\" --description \"Delete an user by ID\" --belongs users --endpoint \"/users/{user_id}\" --public\n</code></pre> <p>Upon executing the Forge command, the <code>delete_user</code> folder will appear within the <code>infra/users</code> directory.</p> <pre><code>functions/users\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 create_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 delete_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 get_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u251c\u2500\u2500 update_user\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"examples/page2/#core-logic_3","title":"Core Logic","text":"<p>Below is the streamlined code for removing a user from DynamoDB using their user ID.</p> functions/users/delete_user/main.py<pre><code>import json\nfrom dataclasses import dataclass\nimport os\nimport boto3\n\n@dataclass\nclass Path:\n    user_id: str\n\n@dataclass\nclass Input:\n    pass\n\n@dataclass\nclass Output:\n    message: str\n\n\ndef lambda_handler(event, context):\n    # Fetch the name of the DynamoDB table from the environment variables.\n    USERS_TABLE_NAME = os.environ.get(\"USERS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource using the boto3 library.\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table by its name.\n    users_table = dynamodb.Table(USERS_TABLE_NAME)\n\n    # Extract the user ID from the path parameters in the event object.\n    user_id = event[\"pathParameters\"].get(\"user_id\")\n\n    # Delete the item with the specified user ID from the DynamoDB table.\n    users_table.delete_item(Key={\"PK\": user_id})\n\n    # Return a response indicating that the user has been successfully deleted, with a 200 HTTP status code.\n    return {\"statusCode\": 200, \"body\": json.dumps({\"message\": \"User deleted\"})}\n</code></pre>"},{"location":"examples/page2/#configuration-class_3","title":"Configuration Class","text":"<p>Here's how to set up the <code>delete user</code> function for interaction with the required AWS resources.</p> functions/users/delete_user/config.py<pre><code>from infra.services import Services\n\n\nclass DeleteUserConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"DeleteUser\",\n            path=\"./functions/users\",\n            description=\"Delete an User\",\n            directory=\"delete_user\",\n            environment={\n                \"USERS_TABLE_NAME\": services.dynamo_db.users_table.table_name,\n            },\n        )\n\n        services.api_gateway.create_endpoint(\n            \"DELETE\", \"/users/{user_id}\", function, public=True\n        )\n\n        services.dynamo_db.users_table.grant_write_data(function)\n</code></pre>"},{"location":"examples/page2/#deploying-our-serverless-crud-application","title":"Deploying Our Serverless CRUD Application","text":"<p>Fantastic, with our four fundamental operations in place, we're ready for deployment to AWS.</p> <p>As a quick refresher, deploying a Lambda Function requires initializing the config class within the LambdaStack class's constructor. Fortunately, Forge automates this process for us. Now, let's examine how our LambdaStack has evolved after our extensive interactions with Forge.</p> infra/stacks/lambda_stack.py<pre><code>from aws_cdk import Stack\nfrom constructs import Construct\nfrom infra.services import Services\nfrom lambda_forge import release\nfrom functions.users.delete_user.config import DeleteUserConfig\nfrom functions.users.update_user.config import UpdateUserConfig\nfrom functions.users.get_user.config import GetUserConfig\nfrom functions.users.create_user.config import CreateUserConfig\nfrom functions.private.config import PrivateConfig\nfrom authorizers.default.config import DefaultAuthorizerConfig\nfrom authorizers.docs.config import DocsAuthorizerConfig\nfrom functions.hello_world.config import HelloWorldConfig\n\n@release\nclass LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # Users\n        DeleteUserConfig(self.services)\n        UpdateUserConfig(self.services)\n        GetUserConfig(self.services)\n        CreateUserConfig(self.services)\n</code></pre> <p>Impressively, Forge has neatly arranged all related Config classes for optimal cohesion.</p> <p>As observed, all four operations have been successfully initialized in our lambda stack, enabling us to move forward by pushing our code to GitHub and awaiting the completion of the CI/CD process. Following this, we should have a fully functional and operational CRUD application at our disposal.</p> <pre><code># Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Developing a CRUD with DynamoDB\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p></p> <p>In this tutorial, the generated base URLs for each environment are:</p> <ul> <li>Dev: <code>https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev</code></li> <li>Staging: <code>https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging</code></li> <li>Prod: <code>https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod</code></li> </ul> <p>For simplicity, we'll focus on demonstrating the processes in the production stage. However, these operations can be similarly conducted using the base URLs for other environments.</p> Prod - Create User<pre><code>curl --request POST \\\n  --url https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/users \\\n  --data '{\n    \"name\": \"John Doe\",\n    \"age\": 30\n}'\n</code></pre> Prod - Get User<pre><code>curl --request GET \\\n  --url https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/users/$USER-ID\n</code></pre> Prod - Update User<pre><code>curl --request PUT \\\n  --url https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/users/$USER-ID \\\n  --data '{\n    \"name\": \"John Doe\",\n    \"age\": 31\n}'\n</code></pre> Prod - Delete User<pre><code>curl --request DELETE \\\n  --url https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/users/$USER-ID\n</code></pre> <p>Congratulations! \ud83c\udf89 You've successfully deployed your very first Serverless application using DynamoDB and Lambda Forge across three different stages! \ud83d\ude80\ud83d\udc69\u200d\ud83d\udcbb</p>"},{"location":"examples/page3/","title":"Building a Serverless Web Scraper with DynamoDB, Lambda Layers, SNS and Event Bridge","text":"<p>In this section, we will develop a serverless web scraper designed to extract informations about books from https://books.toscrape.com/ utilizing the Requests library and Beautiful Soup. The retrieved data will be stored in DynamoDB, enabling us to perform queries via an endpoint.</p> <p>Additionally, we will cover how to configure our Lambda function to execute daily, ensuring our dataset remains current and accurate.</p>"},{"location":"examples/page3/#dynamo-db","title":"Dynamo DB","text":"<p>Considering the write access to our database will be exclusively reserved for the scraper, maintaining three separate databases for each deployment stage is unnecessary. Therefore, let's just create a singular DynamoDB table designed to serve all three environments uniformly.</p> <p>Instead of setting up each environment's details separately in the <code>cdk.json</code> file, like we did to the users table, we'll make things simpler by creating a single Books table on the AWS console and placing its ARN directly into our DynamoDB class.</p> infra/services/dynamo_db.py<pre><code>class DynamoDB:\n  def __init__(self, scope, resources: dict) -&gt; None:\n\n      self.users_table = dynamo_db.Table.from_table_arn(\n          scope,\n          \"UsersTable\",\n          resources[\"arns\"][\"users_table\"],\n      )\n\n      self.books_table = dynamo_db.Table.from_table_arn(\n          scope,\n          \"BooksTable\",\n          \"$BOOKS-TABLE-ARN\",\n      )\n</code></pre>"},{"location":"examples/page3/#lambda-layers","title":"Lambda Layers","text":"<p>Another essential aspect of our project involves leveraging external libraries like <code>requests</code> and <code>Beautiful Soup</code> for our web scraping tasks. Since these libraries are not built into Python's standard library, we'll need to incorporate them into our AWS Lambda functions as Lambda Layers.</p>"},{"location":"examples/page3/#what-are-lambda-layers","title":"What Are Lambda Layers?","text":"<p>Lambda Layers are essentially ZIP archives containing libraries, custom runtime environments, or other dependencies. You can include these layers in your Lambda function\u2019s execution environment without having to bundle them directly with your function's deployment package. This means you can use libraries or custom runtimes across multiple Lambda functions without needing to include them in each function\u2019s codebase.</p>"},{"location":"examples/page3/#incorporating-layers-into-our-service-class","title":"Incorporating Layers Into Our Service Class","text":"<p>Just as we previously set up our DynamoDB Service Class, it's now time to integrate the Lambda Layers into our Service Class using Forge. To accomplish this, simply execute the following command:</p> <p><code>forge service layers</code></p> <p>A new <code>layers.py</code> file has been created on <code>infra/services</code> and automatically incorporated into our Services class. This convenience allows us to focus more on development and less on configuration.</p> <pre><code>infra\n\u251c\u2500\u2500 services\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 api_gateway.py\n    \u251c\u2500\u2500 aws_lambda.py\n    \u251c\u2500\u2500 dynamo_db.py\n    \u2514\u2500\u2500 layers.py\n</code></pre> infra/services/__init__.py<pre><code>from infra.services.layers import Layers\nfrom infra.services.dynamo_db import DynamoDB\nfrom infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\n\n\nclass Services:\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n        self.dynamo_db = DynamoDB(scope, context.resources)\n        self.layers = Layers(scope)\n</code></pre>"},{"location":"examples/page3/#incorporating-requests-and-beautiful-soup-via-public-layers","title":"Incorporating Requests and Beautiful Soup via Public Layers","text":"<p>The <code>requests</code> and <code>Beautiful Soup</code> libraries are widely used and recognized for their utility in web scraping and data extraction tasks. Fortunately, AWS Lambda offers these libraries as public layers, simplifying the process of integrating them into your projects without the need to create custom layers.</p> <p>For projects utilizing Python 3.9, we can leverage the specific Amazon Resource Names (ARNs) for both requests and Beautiful Soup libraries made available through Klayers. This provides an efficient way to add these libraries to your Lambda functions. You can explore the complete list of public layers for Python 3.9 in the <code>us-east-2</code> region here.</p> <p>Here are the ARNs you'll need:</p> <ul> <li> <p>Requests: <code>arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19</code></p> </li> <li> <p>Beautiful Soup 4: <code>arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-beautifulsoup4:7</code></p> </li> </ul> <p>Let's add them both to our Layers class.</p> infra/services/layers.py<pre><code>from aws_cdk import aws_lambda as _lambda\n\n\nclass Layers:\n    def __init__(self, scope) -&gt; None:\n\n        self.requests_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id=\"RequestsLayer\",\n            layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-requests:19\",\n        )\n\n        self.bs4_layer = _lambda.LayerVersion.from_layer_version_arn(\n            scope,\n            id=\"BS4Layer\",\n            layer_version_arn=\"arn:aws:lambda:us-east-2:770693421928:layer:Klayers-p39-beautifulsoup4:7\",\n        )\n</code></pre> <p>Additionally, include the libraries in the <code>requirements.txt</code> file to ensure they are installed during the pipeline execution process.</p> requirements.txt<pre><code>requests==2.28.1\nbeautifulsoup4==4.12.3\n</code></pre>"},{"location":"examples/page3/#developing-the-web-scraper","title":"Developing The Web Scraper","text":"<p>Our web scraper will extract the following details: <code>upc</code>, <code>title</code>, <code>price</code>, <code>category</code>, <code>stock</code>, <code>description</code> and <code>url</code>.</p> <p>Let's create it with forge.</p> <pre><code>forge function scraper --description \"Web scraper to populate Dynamo with books data\" --no-api --belongs books\n</code></pre> <p>Remember, although users can access the scraper's results, the scraper itself won't serve as a direct endpoint. We've included the <code>--no-api</code> flag in our Forge setup to signify that this function won't be connected to the API Gateway. Its primary role is to enrich our database. Additionally, the <code>--belongs</code> flag was used to organize it within the <code>books</code> directory, aligning it with related functions planned for the future.</p> <p>Here is the structure created for the books directory:</p> <pre><code>functions\n\u251c\u2500\u2500 books\n   \u251c\u2500\u2500 scraper\n   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u251c\u2500\u2500 config.py\n   \u2502   \u251c\u2500\u2500 main.py\n   \u2502   \u2514\u2500\u2500 unit.py\n   \u2514\u2500\u2500 utils\n       \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"examples/page3/#building-a-web-scraper-with-pagination-handling-using-a-while-loop","title":"Building a Web Scraper with Pagination Handling Using a While Loop","text":"<p>Our focus is on understanding how AWS resources are integrated with Lambda Forge, not on the intricacies of developing a web scraper. Therefore, we will not cover the source code in detail. Nevertheless, we encourage you to experiment with creating your own web scraper, as the core concepts we're discussing will remain applicable.</p> <p>Below, you'll find the source code accompanied by comments that explain the concepts it illustrates.</p> functions/books/scraper/main.py<pre><code>import os\nimport re\nimport boto3\nimport requests\nfrom bs4 import BeautifulSoup\n\nBASE_URL = \"https://books.toscrape.com\"\n\ndef lambda_handler(event, context):\n\n    # DynamoDB table name for storing books information\n    BOOKS_TABLE_NAME = os.environ.get(\"BOOKS_TABLE_NAME\")\n\n    # Initialize a DynamoDB resource\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Reference the DynamoDB table\n    books_table = dynamodb.Table(BOOKS_TABLE_NAME)\n\n    # Determine the URL to scrape, defaulting to BASE_URL\n    url = event.get(\"url\") or BASE_URL\n\n    while url:\n        # Fetch and parse the webpage at the given URL\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        for article in soup.find_all(\"article\"):\n            # Extract book details\n            title = article.find(\"h3\").find(\"a\").get(\"title\").title()\n            price = article.find(\"p\", {\"class\": \"price_color\"}).get_text()[1:]\n\n            # Correct the href if it doesn't contain \"catalogue/\"\n            href = article.find(\"h3\").find(\"a\").get(\"href\")\n            if \"catalogue/\" not in href:\n                href = f\"catalogue/{href}\"\n\n            # Fetch and parse the book detail page\n            url = f\"{BASE_URL}/{href}\"\n            detail_response = requests.get(url)\n            detail_soup = BeautifulSoup(detail_response.text, \"html.parser\")\n\n            # Extract additional details from the book detail page\n            upc = detail_soup.find(\"th\", string=\"UPC\").find_next(\"td\").get_text().strip()\n            category = (\n                detail_soup.find(\"ul\", {\"class\", \"breadcrumb\"})\n                .find_all(\"li\")[2]\n                .text.strip()\n            )\n            stock = (\n                detail_soup.find(\"p\", {\"class\": \"instock availability\"}).get_text().strip()\n            )\n            stock = re.search(r\"\\d+\", stock)[0]\n            description = detail_soup.find(\"div\", {\"id\": \"product_description\"})\n            if description:\n                description = description.find_next(\"p\").get_text()\n\n            # Construct the item to store in DynamoDB\n            item = {\n                \"PK\": upc,\n                \"category\": category,\n                \"title\": title,\n                \"price\": price,\n                \"description\": description,\n                \"stock\": stock,\n                \"url\": url,\n            }\n\n            # Store the item in DynamoDB\n            books_table.put_item(Item=item)\n\n        # Check for and process the next page\n        next_page = soup.find(\"li\", {\"class\": \"next\"})\n        if next_page:\n            next_href = next_page.find(\"a\")[\"href\"]\n            if \"catalogue/\" not in next_href:\n                next_href = f\"catalogue/{next_href}\"\n            url = f\"{BASE_URL}/{next_href}\"\n        else:\n            url = None\n</code></pre> <p>Due to AWS's predefined operational constraints, Lambda functions are explicitly engineered for rapid execution, with a maximum duration limit of 15 minutes.</p> <p>To evaluate the efficiency of our function, we will incorporate print statements that monitor execution time throughout our local testing phase.</p> <pre><code>Execution time: 1024.913999080658 seconds\n</code></pre> <p>The execution time approaches nearly 17 minutes, exceeding the maximum duration allowed for a Lambda function. Consequently, we need to seek alternative strategies to ensure our scraper remains compliant with the limitations.</p> <p>Utilizing a while loop within a solitary AWS Lambda function to perform book data extraction from the website is functional yet lacks efficiency and scalability. This is particularly pertinent within the AWS ecosystem, which is rich in services tailored for distributed computing and intricate task orchestration.</p>"},{"location":"examples/page3/#building-a-web-scraper-with-pagination-handling-using-sns","title":"Building a Web Scraper with Pagination Handling Using SNS","text":"<p>Amazon Simple Notification Service (SNS) is a fully managed messaging service provided by AWS, enabling seamless communication between distributed systems. It operates on a publish-subscribe model, where messages are published to topics and subscribers receive notifications from these topics. With support for various types of subscriptions including HTTP, SQS, Lambda, email, and SMS, SNS ensures reliable and scalable message delivery across multiple AWS regions. It also offers features like message filtering, retry mechanisms, and dead-letter queues to enhance message processing and system resilience.</p> <p>Instead of using a while loop to process all pages in a single function, let's design a Lambda function to process a maximum of 10 pages. After completing these pages, it should dispatch a message with the URL of the next starting page to an SNS topic. This triggers another Lambda function dedicated to harvesting book information from the subsequent 10 pages.</p> <p>As an initial step, we have to integrate SNS into our Services class.</p> <pre><code>forge service sns\n</code></pre> <p>A new <code>sns.py</code> file was created on <code>infra/services</code>, so create a new SNS topic on the AWS console and place it's ARN on the SNS class.</p> infra/services/sns.py<pre><code>from aws_cdk import aws_lambda_event_sources\nimport aws_cdk.aws_sns as sns\n\n\nclass SNS:\n    def __init__(self, scope, resources, stage) -&gt; None:\n        self.stage = stage\n\n        self.books_scraper_topic = sns.Topic.from_topic_arn(\n            scope,\n            \"BooksScraperTopic\",\n            topic_arn=\"$TOPIC-ARN\",\n        )\n\n    def create_trigger(self, topic, function, stages=None):\n        if stages and self.stage not in stages:\n            return\n        sns_subscription = aws_lambda_event_sources.SnsEventSource(topic)\n        function.add_event_source(sns_subscription)\n</code></pre> <p>Note that the SNS class contains a handy helper method, streamlining the process of establishing triggers that connect an SNS topic to a Lambda function.</p> <p>Now, let's revise the original code to eliminate the while loop that processes all pages and instead publish a message to SNS containing the URL of the new starting point.</p> functions/books/scraper/main.py<pre><code>import os\nimport re\nimport json\nimport time\nimport boto3\nimport requests\nfrom bs4 import BeautifulSoup\n\nBASE_URL = \"https://books.toscrape.com\"\n\ndef lambda_handler(event, context):\n    # Get the DynamoDB table name and SNS topic ARN from environment variables.\n    BOOKS_TABLE_NAME = os.environ.get(\"BOOKS_TABLE_NAME\", \"Books\")\n    SNS_TOPIC_ARN = os.environ.get(\"SNS_TOPIC_ARN\")\n\n    # Initialize the DynamoDB and SNS clients.\n    dynamodb = boto3.resource(\"dynamodb\")\n    sns = boto3.client(\"sns\")\n\n    # Reference the DynamoDB table.\n    books_table = dynamodb.Table(BOOKS_TABLE_NAME)\n\n    # Determine the URL to scrape, defaulting to BASE_URL\n    try:\n        url = json.loads(event['Records'][0]['Sns']['Message'].replace(\"'\", '\"'))[\"url\"]\n    except:\n        url = BASE_URL\n\n    # Keep track of the number of pages processed\n    pages_processed = 0\n\n    # Maximum number of pages to process\n    MAX_PAGES = 10\n\n    while pages_processed &lt; MAX_PAGES:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        for article in soup.find_all(\"article\"):\n            # Extract book details\n            title = article.find(\"h3\").find(\"a\").get(\"title\").title()\n            price = article.find(\"p\", {\"class\": \"price_color\"}).get_text()[1:]\n\n            # Correct the href if it doesn't contain \"catalogue/\"\n            href = article.find(\"h3\").find(\"a\").get(\"href\")\n            if \"catalogue/\" not in href:\n                href = f\"catalogue/{href}\"\n\n            # Fetch and parse the book detail page\n            detail_url = f\"{BASE_URL}/{href}\"\n            detail_response = requests.get(detail_url)\n            detail_soup = BeautifulSoup(detail_response.text, \"html.parser\")\n\n            # Extract additional details from the book detail page\n            upc = detail_soup.find(\"th\", string=\"UPC\").find_next(\"td\").get_text().strip()\n            category = (\n                detail_soup.find(\"ul\", {\"class\", \"breadcrumb\"})\n                .find_all(\"li\")[2]\n                .text.strip()\n            )\n            description = detail_soup.find(\"div\", {\"id\": \"product_description\"})\n            stock = (\n                detail_soup.find(\"p\", {\"class\": \"instock availability\"})\n                .get_text().strip()\n            )\n            stock = re.search(r\"\\d+\", stock)[0]\n            if description:\n                description = description.find_next(\"p\").get_text()\n\n            # Construct the item to store in DynamoDB\n            item = {\n                \"PK\": upc,\n                \"category\": category,\n                \"title\": title,\n                \"price\": price,\n                \"description\": description,\n                \"stock\": stock,\n                \"url\": detail_url,\n            }\n\n            # Store the item in DynamoDB\n            books_table.put_item(Item=item)\n\n        # Increment the number of pages processed\n        pages_processed += 1\n\n        # Check for the next page\n        next_page = soup.find(\"li\", {\"class\": \"next\"})\n        if not next_page:\n            break\n\n        # Correct the href if it doesn't contain \"catalogue/\"\n        next_href = next_page.find(\"a\")[\"href\"]\n        if \"catalogue/\" not in next_href:\n            next_href = f\"catalogue/{next_href}\"\n\n        # Construct the URL for the next page\n        url = f\"{BASE_URL}/{next_href}\"\n\n    if next_page:\n        # Publish a message to the SNS topic to process the next 10 pages\n        sns.publish(\n            TopicArn=SNS_TOPIC_ARN,\n            Message=str({\"url\": url}),\n            Subject=f\"Process next {MAX_PAGES} pages of books\",\n        )\n</code></pre> <p>Let's measure how long that function took to run locally:</p> <pre><code>Execution time: 167.53530287742615 seconds\n</code></pre> <p>Fantastic, it took under 3 minutes!</p> <p>This approach ensures that we never exceed the 15 minutes timeout limit, as each time a new message is published to SNS, the timeout counter is refreshed, allowing continuous execution without interruption.</p>"},{"location":"examples/page3/#configuring-the-web-scraper","title":"Configuring The Web Scraper","text":"<p>Now that we have developed our function, let's proceed to configure the necessary AWS resources for its executions on the cloud.</p> functions/books/scraper/config.py<pre><code>from infra.services import Services\n\n\nclass ScraperConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Scraper\",\n            path=\"./functions/books\",\n            description=\"Web scraper to populate Dynamo with books data\",\n            directory=\"scraper\",\n            timeout=5,\n            layers=[services.layers.requests_layer, services.layers.bs4_layer],\n            environment={\n                \"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name,\n                \"SNS_TOPIC_ARN\": services.sns.books_scraper_topic.topic_arn\n            }\n        )\n\n        services.dynamo_db.books_table.grant_write_data(function)\n\n        services.sns.create_trigger(services.sns.books_scraper_topic, function)\n        services.sns.books_scraper_topic.grant_publish(function)\n</code></pre> <p>This configuration file outlines the setup and permissions for a Lambda function, detailing:</p> <ul> <li>Timeout: Specifies a maximum duration of 5 minutes for Lambda execution.</li> <li>Layers: Adds the requests and bs4 layers to the Lambda function.</li> <li>Environment Variables: Establishes the required environment variables for operation.</li> <li>DynamoDB Access: Provides the Lambda function with write access to the DynamoDB Books table.</li> <li>SNS Trigger: Utilizes the SNS class helper method to link an SNS topic with the production Lambda function.</li> <li>SNS Publishing Permissions: Empowers the Lambda function to publish messages to the books topic.</li> </ul>"},{"location":"examples/page3/#scheduling-executions-with-event-bridge","title":"Scheduling Executions With Event Bridge","text":"<p>The current configuration file equips us to execute the Lambda function as needed. However, it necessitates manual intervention for each run, which is an impractical approach for dynamic tasks like web scraping. The crux of the issue lies in the volatile nature of our target: website data, such as book prices and inventory, can change unpredictably.</p> <p>To mitigate this, we must ensure our web scraper operates automatically at regular intervals, thus capturing updates without manual oversight. By leveraging AWS EventBridge, we can schedule our Lambda function to run periodically, ensuring our data collection remains current with minimal effort.</p> <p>To integrate AWS EventBridge for scheduling tasks, we begin by creating an EventBridge class using Forge. This is achieved with the following command:</p> <pre><code>forge service event_bridge\n</code></pre> <p>After executing the command, a new file named <code>event_bridge.py</code> is generated within the <code>infra/services</code> directory. Let's explore its contents and functionalities:</p> infra/services/event_bridge.py<pre><code>import aws_cdk.aws_events as events\nimport aws_cdk.aws_events_targets as targets\n\n\nclass EventBridge:\n    def __init__(self, scope, resources, stage) -&gt; None:\n        self.scope = scope\n        self.stage = stage\n\n    def create_rule(self, name, expression, target, stages=None):\n        if stages is not None and self.stage not in stages:\n            return\n\n        events.Rule(\n            self.scope,\n            name,\n            schedule=events.Schedule.expression(expression),\n            targets=[targets.LambdaFunction(handler=target)],\n        )\n</code></pre> <p>This class introduces a streamlined method for creating EventBridge rules, enabling the scheduling of Lambda function executions.</p> <p>Before we proceed, it's crucial to acknowledge that we're operating within a multi-stage deployment environment. Our immediate task involves configuring the Scraper function to activate based on a scheduled rule. However, a pertinent question arises: Should we initiate the triggering of three distinct functions simultaneously? Of course not, especially when considering efficiency and resource management. More precisely, is there a need for three separate scrapers when, in reality, only one scraper is destined for activation?</p> <p>Bearing this consideration in mind, it's wise to implement a few minor adjustments. Our goal is to streamline the process, thereby avoiding the unnecessary creation of unused scrapers.</p> <p>First, let's modify the <code>LambdaStack</code> class to send also the context to the <code>ScraperConfig</code> class.</p> infra/stacks/lambda_stack.py<pre><code>        # Books\n        ScraperConfig(self.services, context)\n</code></pre> <p>Now, let's modify our configuration class to accept the <code>context</code> as an additional argument in its constructor.</p> <p>By incorporating the context, we can strategically condition the creation of the function based on the deployment stage.</p> functions/books/scraper/config.py<pre><code>from infra.services import Services\n\n\nclass ScraperConfig:\n    def __init__(self, services: Services, context) -&gt; None:\n\n        if context.stage != \"Prod\":\n            return\n\n        function = services.aws_lambda.create_function(\n            name=\"Scraper\",\n            path=\"./functions/books\",\n            description=\"Web scraper to populate Dynamo with books data\",\n            directory=\"scraper\",\n            timeout=5,\n            layers=[services.layers.requests_layer, services.layers.bs4_layer],\n            environment={\n                \"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name,\n                \"SNS_TOPIC_ARN\": services.sns.books_scraper_topic.topic_arn\n            }\n        )\n\n        services.dynamo_db.books_table.grant_write_data(function)\n\n        services.sns.create_trigger(services.sns.books_scraper_topic, function)\n        services.sns.books_scraper_topic.grant_publish(function)\n\n        services.event_bridge.create_rule(\n            name=\"ScraperRule\",\n            expression=\"cron(0 12 ? * * *)\",\n            target=function,\n        )\n</code></pre> <p>The cron expression <code>cron(0 12 ? * * *)</code> configures a schedule to initiate an action every day at 12 PM UTC.</p> <p>Now, we're streamlining our deployment by creating the Lambda function and its essential resources exclusively for the staging environment that will be actively utilized.</p>"},{"location":"examples/page3/#developing-an-endpoint-for-data-access","title":"Developing an Endpoint for Data Access","text":"<p>Let's create an endpoint that returns all the stored data from our database or allows filtering by category, facilitating easy access and manipulation of the data.</p> <pre><code>forge function list_books --method \"GET\" --description \"A function to fetch books from DynamoDB, optionally filtered by category.\" --belongs books --public\n</code></pre> <p>The file has been created within the <code>books</code> directory, as initially planned.</p> <pre><code>functions\n\u251c\u2500\u2500 books\n    \u251c\u2500\u2500 list_books\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u251c\u2500\u2500 integration.py\n    \u2502   \u251c\u2500\u2500 main.py\n    \u2502   \u2514\u2500\u2500 unit.py\n    \u251c\u2500\u2500 scraper\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config.py\n    \u2502   \u251c\u2500\u2500 main.py\n    \u2502   \u2514\u2500\u2500 unit.py\n    \u2514\u2500\u2500 utils\n        \u2514\u2500\u2500 __init__.py\n</code></pre> <p>To optimize data retrieval by category from our DynamoDB table, we need to create a Global Secondary Index (GSI) on the Books Table. This index enables efficient querying and filtering of data based on the <code>category</code> attribute, without the need for scanning the entire table.</p> <p>Go to the DynamoDB section within the AWS Management Console and select the Books Table. Click on the <code>Indexes</code> tab next to the table details, then press <code>Create index</code>. In the creation form, set the partition key to your <code>category</code> column. Name your index as <code>CategoryIndex</code>. After configuring these details, review your settings and confirm by clicking <code>Create index</code>.</p> <p>Having established our index, we can utilize it to precisely and efficiently fetch data by category when needed, significantly optimizing our query performance.</p> functions/books/list_books/main.py<pre><code>import json\nimport os\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nimport boto3\nfrom boto3.dynamodb.conditions import Key\n\n@dataclass\nclass Input:\n    category: Optional[str]\n\n@dataclass\nclass Book:\n    id: str\n    title: str\n    price: str\n    category: str\n    stock: str\n    description: str\n    url: str\n\n@dataclass\nclass Output:\n    data: List[Book]\n\ndef lambda_handler(event, context):\n    # Initialize a DynamoDB client\n    dynamodb = boto3.resource(\"dynamodb\")\n\n    # Get the name of the table from the environment variable\n    BOOKS_TABLE_NAME = os.environ[\"BOOKS_TABLE_NAME\"]\n\n    # Create a DynamoDB table resource\n    table = dynamodb.Table(BOOKS_TABLE_NAME)\n\n    # Check if a category is specified in the query string parameters\n    category = (\n        event[\"queryStringParameters\"].get(\"category\")\n        if event[\"queryStringParameters\"]\n        else None\n    )\n\n    processed_items = []\n    last_evaluated_key = None\n\n    # Handle pagination\n    while True:\n        scan_kwargs = {}\n        if category:\n            scan_kwargs.update({\n                'IndexName': \"CategoryIndex\",\n                'KeyConditionExpression': Key(\"category\").eq(category.title())\n            })\n\n        if last_evaluated_key:\n            scan_kwargs['ExclusiveStartKey'] = last_evaluated_key\n\n        if category:\n            response = table.query(**scan_kwargs)\n        else:\n            response = table.scan(**scan_kwargs)\n\n        items = response.get(\"Items\", [])\n\n        # Renaming 'PK' attribute to 'id' in each item\n        processed_items.extend(\n            [{\"id\": item[\"PK\"], **{k: v for k, v in item.items() if k != \"PK\"}} for item in items]\n        )\n\n        last_evaluated_key = response.get('LastEvaluatedKey')\n        if not last_evaluated_key:\n            break\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"data\": processed_items})}\n</code></pre> <p>Now Let's configure the function</p> functions/books/list_books/config.py<pre><code>from infra.services import Services\n\n\nclass ListBooksConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"ListBooks\",\n            path=\"./functions/books\",\n            description=\"A function to fetch books from DynamoDB, optionally filtered by category.\",\n            directory=\"list_books\",\n            environment={\"BOOKS_TABLE_NAME\": services.dynamo_db.books_table.table_name},\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/books\", function, public=True)\n\n        services.dynamo_db.books_table.grant_read_data(function)\n        services.dynamo_db.add_query_permission(\n            function, services.dynamo_db.books_table\n        )\n</code></pre> <p>In addition to the foundational setup facilitated by Forge, this configuration file plays a crucial role in further customizing our function. It specifically focuses on defining environment variables and granting read permissions to the function for accessing the Books table.</p> <p>Moreover, we leverage a specialized helper method within our DynamoDB class to extend Query permissions to the Lambda function. This distinction is critical as querying entails more specific privileges beyond data reading, ensuring our function has the precise access needed for optimal operation.</p>"},{"location":"examples/page3/#launching-our-web-scraper-and-data-visualization-endpoint","title":"Launching Our Web Scraper and Data Visualization Endpoint","text":"<p>Great, we're all set to deploy our function.</p> <p>Now, we'll commit and push our changes to the remote repository, allowing our CI/CD pipeline to handle the deployment seamlessly.</p> <pre><code># Add changes to the staging area\ngit add .\n\n# Commit the changes with a descriptive message\ngit commit -m \"Deploying Web Scraper and Data Visualization Endpoint\"\n\n# Push changes to the 'dev' branch.\ngit push origin dev\n\n# Switch to the 'staging' branch, merge changes from 'dev', and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Switch to the 'main' branch, merge changes from 'staging', and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>Once the pipeline execution concludes, expect to see a single scraper function established.</p> <p></p> <p>Additionally, this function will be configured with two distinct triggers: an SNS trigger and an Event Bridge trigger, each serving a unique purpose in the workflow.</p> <p></p> <p>Now we can also test new endpoints to list the scraped data.</p> <ul> <li>Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/books</li> <li>Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/books</li> <li>Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/books</li> </ul> <p>Congratulations! \ud83c\udf89 You've successfully created your first web scraper using Lambda Layers, SNS, DynamoDB and Event Bridge using Lambda Forge. \ud83d\ude80</p>"},{"location":"home/page1/","title":"Introduction","text":""},{"location":"home/page1/#welcome-to-lambda-forge","title":"Welcome to Lambda Forge","text":"<p>Lambda Forge is an open-source Python framework, built on top of AWS Cloud Development Kit, that revolutionizes the AWS Lambda deployment enabling a modular and scalable architecture with an automated CI/CD pipeline for a multi-stage environment.</p> <p>Additionally, it includes a Command Line Interface named <code>Forge</code>, designed to optimize your development workflow. Forge not only speeds up and standardizes the development process but also automatically generates comprehensive documentation for your endpoints using Swagger and Redoc.</p> <p></p>"},{"location":"home/page2/","title":"Getting Started","text":""},{"location":"home/page2/#install-and-configure-aws-cdk","title":"Install and Configure AWS CDK","text":"<p>Lambda Forge is built on top of AWS Cloud Development Kit (CDK) and it's essential for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. Execute the following commands to install the AWS CDK globally and set up your AWS credentials:</p> <pre><code>npm install -g aws-cdk\naws configure\n</code></pre> <p>During the configuration, you will be prompted to enter your AWS Access Key ID, Secret Access Key, default region name, and output format.</p>"},{"location":"home/page2/#create-a-github-personal-access-token","title":"Create a GitHub Personal Access Token","text":"<p>Lambda Forge uses CodePipeline to interact with your GitHub repository. To enable this, generate a GitHub personal access token by following these steps:</p> <ol> <li>Navigate to \"Developer Settings\" in your GitHub account.</li> <li>Select \"Personal access tokens,\" then \"Tokens (classic).\"</li> <li>Click \"Generate new token,\" ensuring the \"repo\" scope is selected for full control of private repositories.</li> <li>Complete the token generation process.</li> </ol> <p>You can find more informations about creating a GitHub Token here.</p> <p>Your token will follow this format: <code>ghp_********************************</code></p>"},{"location":"home/page2/#store-the-token-on-aws-secrets-manager","title":"Store the Token on AWS Secrets Manager","text":"<p>Save this token in AWS Secrets Manager as <code>plain text</code> using the exact name github-token. This specific naming is vital as it corresponds to the default identifier that the CDK looks for within your AWS account.</p>"},{"location":"home/page2/#create-a-new-directory","title":"Create a New Directory","text":"<pre><code>mkdir lambda_forge_demo\ncd lambda_forge_demo\n</code></pre>"},{"location":"home/page2/#create-a-new-virtual-environment","title":"Create a New Virtual Environment","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"home/page2/#install-lambda-forge","title":"Install lambda-forge","text":"<pre><code>pip install lambda-forge --extra-index-url https://pypi.org/simple --extra-index-url https://test.pypi.org/simple/\n</code></pre>"},{"location":"home/page2/#forge-cli","title":"Forge CLI","text":"<p>The Forge Command Line Interface (CLI) is a powerful, versatile tool designed to streamline the development, deployment, and management of applications. It enables developers to automate repetitive tasks, manage project configurations, and interact directly with the services and infrastructure without leaving the terminal. This CLI tool simplifies complex processes through straightforward commands, significantly reducing development time and effort.</p>"},{"location":"home/page2/#verify-installation","title":"Verify Installation","text":"<p>Having successfully installed Lambda Forge, you are now ready to explore the capabilities of the Forge CLI. Begin by entering the following command to access the comprehensive list of available options and commands:</p> <pre><code>forge --help\n</code></pre>"},{"location":"home/page2/#create-a-new-project","title":"Create a New Project","text":"<p>Start a new project named <code>lambda-forge-demo</code>, incorporating the <code>--no-docs</code> flag to bypass docs generation initially as this will be covered on a future section.</p> <pre><code>forge project lambda-forge-demo --repo-owner \"$GITHUB-OWNER\" --repo-name \"$GITHUB-REPO\" --no-docs\n</code></pre> <p>Make sure to replace <code>$GITHUB-OWNER</code> and <code>$GITHUB-REPO</code> with the actual GitHub owner and the name of an empty repository.</p>"},{"location":"home/page2/#project-structure","title":"Project Structure","text":"<p>Upon creatig your project, some directories and files are automatically generated for you. This initial structure is designed to streamline the setup process and provide a solid foundation for further development.</p> <p>In the upcoming sections of this tutorial, we'll explore each of these components in detail. For now, familiarize yourself with the foundational structure that should resemble the following:</p> <pre><code>.\n\u251c\u2500\u2500 authorizers\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 functions\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 infra\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 services\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 api_gateway.py\n\u2502   \u2502   \u2514\u2500\u2500 aws_lambda.py\n\u2502   \u251c\u2500\u2500 stacks\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 dev_stack.py\n\u2502   \u2502   \u251c\u2500\u2500 lambda_stack.py\n\u2502   \u2502   \u251c\u2500\u2500 prod_stack.py\n\u2502   \u2502   \u2514\u2500\u2500 staging_stack.py\n\u2502   \u2514\u2500\u2500 stages\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 deploy.py\n\u251c\u2500\u2500 .coveragerc\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 cdk.json\n\u251c\u2500\u2500 pytest.ini\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>The <code>cdk.json</code> file, located at the root of your directory, serves as the central configuration hub for Lambda Forge projects. When you run the <code>forge project</code> command, Forge automatically applies the informed settings into the cdk.json file.</p> cdk.json<pre><code>    \"region\": \"us-east-2\",\n    \"account\": \"\",\n    \"name\": \"Lambda-Forge-Demo\",\n    \"repo\": {\n      \"owner\": \"$GITHUB-OWNER\",\n      \"name\": \"$GITHUB-REPO\"\n    },\n    \"bucket\": \"\",\n    \"coverage\": 80,\n</code></pre> <p>For a comprehensive list of configurations that Forge supports, you can refer to the command line help by running <code>forge project --help</code>.</p>"},{"location":"home/page3/","title":"Creating a Public Hello World Function With API Gateway","text":"<p>Creating a public \"Hello World\" function is a fantastic way to get started with Lambda Forge. This function will serve as a simple demonstration of Lambda Forge's ability to quickly deploy serverless functions accessible via an HTTP endpoint.</p> <p>Here's how you can create your first public Hello World function.</p> <pre><code>forge function hello_world --method \"GET\" --description \"A simple hello world\" --public\n</code></pre> <p>This command prompts Lambda Forge to initiate a new Lambda function located in the <code>hello_world</code> directory. The <code>--method</code> parameter defines the HTTP method accessible for this function.. The <code>--description</code> option provides a concise summary of the function\u2019s intent, and the <code>--public</code> flag ensures the function is openly accessible, allowing it to be invoked by anyone who has the URL.</p>"},{"location":"home/page3/#understanding-the-function-structure","title":"Understanding the Function Structure","text":"<p>When you create a new function with Lambda Forge, it not only simplifies the creation process but also sets up a robust and organized file structure for your function. This structure is designed to support best practices in software development, including separation of concerns, configuration management, and testing. Let's break down the structure of the automatically generated hello_world function:</p> <pre><code>functions/\n\u2514\u2500\u2500 hello_world/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre> <ul> <li><code>functions/</code> This directory is the root folder for all your Lambda functions. Each function has its own subdirectory within this folder.</li> <li><code>hello_world/</code> The hello_world subdirectory contains all the necessary files for your function to run, be configured, and tested.</li> <li><code>__init__.py</code> This file marks the directory as a Python package, allowing its modules to be imported elsewhere.</li> <li><code>config.py</code> Holds the configuration settings for the function. These might include environment variables, resource identifiers, and other parameters critical for the function's operation.</li> <li><code>integration.py</code> Contains integration tests that simulate the interaction of your function with external services or resources.</li> <li><code>main.py</code> This is where the core logic of your Lambda function resides. The handler function, which AWS Lambda invokes when the function is executed, is defined here.</li> <li><code>unit.py</code> Contains unit tests for your function. Unit tests focus on testing individual parts of the function's code in isolation, ensuring that each component behaves as expected.</li> </ul>"},{"location":"home/page3/#implementing-the-hello-world-function","title":"Implementing the Hello World Function","text":"<p>The Lambda function's implementation should be in the <code>main.py</code> file. Below is an example showcasing our simple HelloWorld function:</p> functions/hello_world/main.py<pre><code>import json\nfrom dataclasses import dataclass\n\n@dataclass\nclass Input:\n    pass\n\n@dataclass\nclass Output:\n    message: str\n\ndef lambda_handler(event, context):\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"Hello World!\"})\n    }\n</code></pre> <p>The <code>Input</code> and <code>Output</code> data classes are the entrypoint for the documentation creation process. However, since the project was launched with the <code>--no-docs</code> flag, we will temporarily skip the docs generation details.</p> <p>Moving forward, we've successfully implemented a straightforward lambda function that outputs a basic JSON response: <code>{\"message\": \"Hello World!\"}</code>.</p>"},{"location":"home/page3/#configuring-your-lambda-function-dependencies","title":"Configuring Your Lambda Function Dependencies","text":""},{"location":"home/page3/#the-services-class","title":"The Services Class","text":"<p>Within the <code>infra/services/__init__.py</code> file, you'll find the Services class, a comprehensive resource manager designed to streamline the interaction with AWS services. This class acts as a dependency injector, enabling the easy and efficient configuration of AWS resources directly from your <code>config.py</code> files.</p> infra/services/__init__.py<pre><code>from infra.services.api_gateway import APIGateway\nfrom infra.services.aws_lambda import AWSLambda\n\nclass Services:\n\n    def __init__(self, scope, context) -&gt; None:\n        self.api_gateway = APIGateway(scope, context)\n        self.aws_lambda = AWSLambda(scope, context)\n</code></pre>"},{"location":"home/page3/#utilizing-the-services-class-in-configpy","title":"Utilizing the Services Class in config.py","text":"<p>In our Lambda Forge projects, the <code>config.py</code> file plays a crucial role in defining and configuring the dependencies required by a Lambda function.</p> <p>By passing an instance of Services to our configuration classes, we can seamlessly create and manage resources such as Lambda functions and API Gateway endpoints.</p> functions/hello_world/config.py<pre><code>from infra.services import Services\n\nclass HelloWorldConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"HelloWorld\",\n            path=\"./functions/hello_world\",\n            description=\"A simple hello world\"\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/hello_world\", function, public=True)\n</code></pre> <p>The Forge CLI has significantly simplified the setup by automatically tailoring the function to meet our specifications. Essentially, the <code>config.py</code> file configures a Lambda Function to be named as <code>HelloWorld</code> accompanied by the description <code>A simple hello world</code>.</p> <p>Additionally, it sets up the function to respond to GET requests at the <code>/hello_world</code> path and designates it as a public endpoint, making it accessible without authentication.</p>"},{"location":"home/page3/#deploying-your-lambda-function","title":"Deploying Your Lambda Function","text":"<p>To deploy your Lambda function, you should integrate the Config class within the <code>infra/stacks/lambda_stack.py</code> file.</p> <p>The Forge CLI streamlines this process by automatically incorporating it for you.</p> infra/stacks/lambda_stack.py<pre><code>from aws_cdk import Stack\nfrom constructs import Construct\nfrom infra.services import Services\nfrom lambda_forge import release\nfrom functions.hello_world.config import HelloWorldConfig\n\n\n@release\nclass LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # HelloWorld\n        HelloWorldConfig(self.services)\n</code></pre>"},{"location":"home/page3/#push-your-code-to-github","title":"Push Your Code To Github","text":"<p>With all the required settings now in place, we're ready to upload our code to the GitHub repository.</p> <p>Lambda Forge is designed to support a multi-stage deployment process, automatically creating environments for Production, Staging and Development. These environments correspond to the <code>main</code>, <code>staging</code>, and <code>dev</code> branches, respectively.</p> <p>For the sake of simplicity, we'll focus on deploying only the development branch at this moment, deferring the discussion on setting up a multi-stage environment to a future session.</p> <pre><code># Initialize the Git repository\ngit init\ngit add .\n\n# Commit the changes\ngit commit -m \"Initial commit\"\n\n# Set the remote repository\ngit remote add origin git@github.com:$GITHUB_USER/$GITHUB_REPO.git\n\n# Create, checkout, and push the 'dev' branch\ngit checkout -b dev\ngit push -u origin dev\n</code></pre>"},{"location":"home/page3/#deploying-the-stacks","title":"Deploying the Stacks","text":"<p>Lambda Forge ensures that every resource it creates on AWS follows a naming convention that integrates the deployment stage, the project name, and the resource name. This approach guarantees a consistent and clear identification methodology throughout the project.</p> <p>The project name is defined within the <code>cdk.json</code> file, linking each resource directly to its associated project and stage for easy management and recognition.</p> cdk.json<pre><code>    \"region\": \"us-east-2\",\n    \"account\": \"\",\n    \"name\": \"Lambda-Forge-Demo\",\n    \"repo\": {\n      \"owner\": \"$GITHUB-OWNER\",\n      \"name\": \"$GITHUB-REPO\"\n    },\n</code></pre> <p>Deploy the Dev Stack by running the following command in your terminal:</p> <pre><code>cdk deploy Dev-Lambda-Forge-Demo-Stack\n</code></pre> <p>Following a successful deployment, a new pipeline will be created with the name <code>Dev-Lambda-Forge-Demo-Pipeline</code>. Access your AWS CodePipeline console to view it.</p> <p></p> <p>In a dedicated session, we'll delve into the specifics of the pipelines generated, including a closer examination of the development pipeline.</p> <p>By default, Lambda Forge does not incorporate any steps for code validation in the dev pipeline. Instead, it seamlessly integrates Github with AWS CodePipeline. This means that once code is pushed to GitHub, it triggers the pipeline, leading to automatic deployment upon the completion of the execution process.</p> <p>After the pipeline execution concludes, proceed to your AWS Lambda console and locate the <code>Dev-Lambda-Forge-Demo-HelloWorld</code> function.</p> <p></p> <p>Select the function, then navigate to <code>Configurations -&gt; Triggers</code>. Here, you will be presented with a link to your newly deployed Lambda function, ready for use.</p> <p></p> <p>For this tutorial, the Lambda function is accessible via the following URL:</p> <ul> <li>https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/hello_world</li> </ul> <p>Congratulations! \ud83c\udf89 You've successfully deployed your very first Hello World function using Lambda Forge! \ud83d\ude80</p>"},{"location":"home/page4/","title":"Securing Endpoints Through an Authorizer","text":"<p>In this section, we will delve into securing endpoints by introducing an intermediary function known as an authorizer which will be responsible for validating incoming requests, determining if they should be allowed to access the targeted resources.</p> <p>By implementing an authorizer, you can ensure that only authenticated and authorized requests are processed by your endpoints, enhancing the security and privacy of your application.</p> <p>In fact, Lambda Forge treats all lambda functions as private by default. That's why we had to use the <code>--public</code> flag when creating the previous hello world function, to make it accessible without authentication. Without this flag, we would have been required to implement an authorizer for user authentication.</p>"},{"location":"home/page4/#creating-an-authorizer","title":"Creating an Authorizer","text":"<p>First, let's begin by creating a new authorizer function with the following command:</p> <pre><code>forge authorizer secret --description \"An authorizer to validate requests based on a secret present on the headers\"\n</code></pre> <p>This command instructs the forge CLI tool to create a new authorizer under the <code>secret</code> directory.</p>"},{"location":"home/page4/#authorizer-structure","title":"Authorizer Structure","text":"<p>Authorizers, while closely resembling Lambda Functions in structure, they fulfill a distinct role.</p> <p>Let's examine the structure of an authorizer more closely:</p> <pre><code>authorizers\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 secret\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 utils\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <ul> <li><code>authorizers/</code> This directory serves as the central hub for all authorizer functions, analogous to how the <code>functions/</code> directory houses Lambda functions. Each distinct authorizer is allocated its own subdirectory within this folder.</li> <li><code>secret/</code> This subdirectory is specifically designed for developing the <code>secret</code> authorizer.</li> <li><code>__init__.py</code> Marks the directory as a Python package, enabling its modules to be imported elsewhere within the project.</li> <li><code>config.py</code> Contains the configuration settings for the authorizer, such as environmental variables and access control parameters.</li> <li><code>main.py</code> Houses the main logic for the authorizer, detailing how incoming requests are verified.</li> <li><code>unit.py</code> Focused on unit testing for the authorizer, these tests ensure that each part of the authorizer's code operates as expected independently.</li> <li><code>utils/</code> Provides utility functions that are used by the authorizers, offering common functionalities or resources that can be leveraged across various authorizers.</li> </ul>"},{"location":"home/page4/#implementing-the-authorizer","title":"Implementing The Authorizer","text":"<p>Forge automatically generates a basic implementation of an AWS Lambda authorizer. This example is intended solely for demonstration and learning purposes, and it is critical to devise a comprehensive and secure authentication mechanism suitable for your application's specific security needs. For demonstration, the authorizer checks a custom header for a specific secret value to decide on granting or denying access.</p> <p>Important Note: The example below employs a simple secret key for authorization and should not be used in production environments. It is crucial to replace this logic with a robust, secure authorization strategy before deploying your application.</p> authorizers/secret/main.py<pre><code>def lambda_handler(event, context):\n\n    # ATTENTION: The example provided below is strictly for demonstration purposes and should NOT be deployed in a production environment.\n    # It's crucial to develop and integrate your own robust authorization mechanism tailored to your application's security requirements.\n    # To utilize the example authorizer as a temporary placeholder, ensure to include the following header in your requests:\n\n    # Header:\n    # secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8\n\n    # Remember, security is paramount. This placeholder serves as a guide to help you understand the kind of information your custom authorizer should authenticate.\n    # Please replace it with your secure, proprietary logic before going live. Happy coding!\n\n    secret = event[\"headers\"].get(\"secret\")\n\n    SECRET = \"CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8\"\n    effect = \"allow\" if secret == SECRET else \"deny\"\n\n    policy = {\n        \"policyDocument\": {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Action\": \"execute-api:Invoke\",\n                    \"Effect\": effect,\n                    \"Resource\": \"*\"\n                }\n            ],\n        },\n    }\n    return policy\n</code></pre> <p>The code snippet above demonstrates that the authorizer is configured to verify the presence of a specific header in the request, as shown below:</p> <p><code>secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8</code></p> <p>This key serves as a simple form of authentication, granting or denying access based on its presence and accuracy in the request headers.</p> <p>The secret mentioned is automatically generated by Forge, meaning the specific secret you encounter during your implementation will differ from the example provided. Please be mindful of this distinction as you proceed.</p>"},{"location":"home/page4/#configuring-the-authorizer","title":"Configuring The Authorizer","text":"<p>Similar to lambda functions in terms of setup, authorizers diverge in their application. Instead of establishing an endpoint on API Gateway, an authorizer is configured to control access to one or more endpoints.</p> authorizers/secret/config.py<pre><code>from infra.services import Services\n\nclass SecretAuthorizerConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"SecretAuthorizer\",\n            path=\"./authorizers/secret\",\n            description=\"An authorizer to validate requests based on a secret present on the headers\"\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"secret\")\n</code></pre> <p>The configuration detailed above establishes a new authorizer, assigning it a unique identifier <code>secret</code> within the API Gateway.</p>"},{"location":"home/page4/#adding-authorizer-to-lambda-stack","title":"Adding Authorizer To Lambda Stack","text":"<p>Similarly to the functions, an authorizer needs to be initialized within the <code>LambdaStack</code> class.</p> <p>Fortunately, Forge takes care of this automatically.</p> infra/stacks/lambda_stack.py<pre><code>from aws_cdk import Stack\nfrom constructs import Construct\nfrom infra.services import Services\nfrom lambda_forge import release\nfrom authorizers.secret.config import SecretAuthorizerConfig\nfrom functions.hello_world.config import HelloWorldConfig\n\n\n@release\nclass LambdaStack(Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n\n        super().__init__(scope, f\"{context.name}-Lambda-Stack\", **kwargs)\n\n        self.services = Services(self, context)\n\n        # Authorizers\n        SecretAuthorizerConfig(self.services)\n\n        # HelloWorld\n        HelloWorldConfig(self.services)\n</code></pre>"},{"location":"home/page4/#creating-a-private-function","title":"Creating a Private Function","text":"<p>Now let's create a new private function.</p> <pre><code>forge function private --method \"GET\" --description \"A private function\"\n</code></pre> <p>Upon creating a new function using the Forge CLI, the project's function structure is expanded to include this new function alongside the existing ones.</p> <pre><code>functions\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 hello_world\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 integration.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 unit.py\n\u2514\u2500\u2500 private\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 integration.py\n    \u251c\u2500\u2500 main.py\n    \u2514\u2500\u2500 unit.py\n</code></pre>"},{"location":"home/page4/#implementing-the-function","title":"Implementing the Function","text":"<p>Let's make some adjustments to the response returned by this Lambda function:</p> functions/private/main.py<pre><code>def lambda_handler(event, context):\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\"message\": \"Hello From Private!\"})\n    }\n</code></pre> <p>Rather than displaying the message <code>Hello World!</code>, we will now return <code>Hello From Private!</code>.</p> <p>Additionally, let's revise the unit tests to accurately represent the modifications we've implemented in our code.</p> functions/private/unit.py<pre><code>import json\nfrom .main import lambda_handler\n\ndef test_lambda_handler():\n\n    response = lambda_handler(None, None)\n\n    assert response[\"body\"] == json.dumps({\"message\": \"Hello From Private!\"})\n</code></pre>"},{"location":"home/page4/#configuring-the-function-as-private","title":"Configuring the Function as Private","text":"<p>To configure the function as private, we must link it to the authorizer by passing the authorizer's name, established during its creation, to the <code>create_endpoint</code> method.</p> functions/private/config.py<pre><code>from infra.services import Services\n\nclass PrivateConfig:\n    def __init__(self, services: Services) -&gt; None:\n\n        function = services.aws_lambda.create_function(\n            name=\"Private\",\n            path=\"./functions/private\",\n            description=\"A private function\",\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/private\", function, authorizer=\"secret\")\n</code></pre> <p>This configuration file establishes a new private function that is secured with the <code>secrets</code> authorizer and accessible via a GET request at the <code>/private</code> endpoint.</p>"},{"location":"home/page4/#deployment-process-for-both-authorizer-and-function","title":"Deployment Process for Both Authorizer and Function","text":"<p>As the next step, let's proceed to upload our updates to GitHub.</p> <pre><code># Add all changes to the staging area\ngit add .\n\n# Commit the staged changes with a clear message\ngit commit -m \"Implemented a private function with an authorizer\"\n\n# Push the committed changes to the 'dev' branch\ngit push origin dev\n</code></pre> <p>This operation will automatically initiate our development pipeline.</p> <p></p> <p>After the pipeline completes successfully, the private Lambda function becomes operational:</p> <ul> <li>Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/private</li> </ul> <p>Direct access to these URLs through a web browser will display an unauthorized access message:</p> <pre><code>{\n  \"Message\": \"User is not authorized to access this resource with an explicit deny\"\n}\n</code></pre> <p>However, access is granted when including the necessary secret in the request header. Below is how to use <code>curl</code> to access the Lambda function:</p> <pre><code>curl --request GET \\\n  --url https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/private \\\n  --header 'secret: CRMdDRMA4iW4xo9l38pACls7zsHYfp8T7TLXtucysb2lB5XBVFn8'\n</code></pre> <p>Upon running the curl command, you will receive the following response:</p> <pre><code>{\n  \"message\": \"Hello From Private!\"\n}\n</code></pre> <p>This validates the functionality of our authorizer, effectively securing the private Lambda function to ensure access is only available to those possessing the correct secret header.</p>"},{"location":"home/page4/#setting-a-default-authorizer","title":"Setting a Default Authorizer","text":"<p>Lambda Forge automatically considers all functions as private unless specified otherwise. This means functions are generally expected to require an authorizer for access control, unless they are explicitly marked as public.</p> <p>To facilitate easier management and to obviate the need for specifying an authorizer for each Lambda function individually, Lambda Forge allows for the designation of a default authorizer. This default authorizer is automatically applied to all non-public Lambda functions, streamlining the configuration process for securing access.</p> <p>To set an authorizer as the default, you can use the <code>default=True</code> argument in the <code>create_authorizer</code> method when defining your authorizer.</p> authorizers/secret/config.py<pre><code>        function = services.aws_lambda.create_function(\n            name=\"SecretAuthorizer\",\n            path=\"./authorizers/secret\",\n            description=\"An authorizer to validate requests based on a secret present on the headers\"\n        )\n\n        services.api_gateway.create_authorizer(function, name=\"secret\", default=True)\n</code></pre> <p>Next, we'll update the Private Function configuration to no longer directly associate it with the <code>secrets</code> authorizer.</p> functions/private/config.py<pre><code>        function = services.aws_lambda.create_function(\n            name=\"Private\",\n            path=\"./functions/private\",\n            description=\"A private function\",\n        )\n\n        services.api_gateway.create_endpoint(\"GET\", \"/private\", function)\n</code></pre> <p>Having designated the <code>secret</code>authorizer as the default, any function not explicitly linked to a particular authorizer and not flagged as public, such as this one, will inherently be protected by the <code>secret</code> authorizer by default.</p>"},{"location":"home/page5/","title":"Multi-Stage Environments With AWS CodePipeline","text":"<p>In practical scenarios, it is highly recommended to adopt a multi-stage development approach. This strategy allows you to freely develop and test your code in isolated environments without affecting your live production environment and, consequently, the real-world users of your application.</p> <p>In Lambda Forge, the pipelines for development, staging, and production are meticulously organized within distinct files, found at <code>infra/stacks/dev_stack.py</code>, <code>infra/stacks/staging_stack.py</code>, and <code>infra/stacks/prod_stack.py</code>, respectively.</p> <p>Each stage is designed to operate with its own set of isolated resources, to ensure that changes in one environment do not inadvertently affect another.</p> <p>Note</p> Lambda Forge provides a suggested pipeline configuration for each stage of deployment. You're encouraged to customize these pipelines to fit your project's needs. Whether adding new steps, adjusting existing ones, reordering or even removing some of them."},{"location":"home/page5/#development-environment","title":"Development Environment","text":"<p>The <code>Development</code> environment is where the initial coding and feature implementation occur, allowing developers to make frequent changes and test new ideas in an isolated environment.</p> <p>This environment is strategically structured to facilitate rapid deployments, allowing new features to be rolled out directly without undergoing any preliminary validation steps. It functions essentially as a sandbox environment, providing developers with a space to both develop and test new features in a fast-paced and flexible setting. This approach enables immediate feedback and iterative improvements, streamlining the development process.</p>"},{"location":"home/page5/#configuring-the-development-environment","title":"Configuring the Development Environment","text":"<p>This section details the setup process for the development environment.</p> infra/stacks/dev_stack.py<pre><code>import aws_cdk as cdk\nfrom aws_cdk import pipelines as pipelines\nfrom aws_cdk.pipelines import CodePipelineSource\nfrom constructs import Construct\nfrom lambda_forge import context\n\nfrom infra.stages.deploy import DeployStage\n\n\n@context(stage=\"Dev\", resources=\"dev\")\nclass DevStack(cdk.Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n        super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs)\n\n        source = CodePipelineSource.git_hub(\n            f\"{context.repo['owner']}/{context.repo['name']}\", \"dev\"\n        )\n\n        pipeline = pipelines.CodePipeline(\n            self,\n            \"Pipeline\",\n            synth=pipelines.ShellStep(\n                \"Synth\",\n                input=source,\n                install_commands=[\n                    \"pip install lambda-forge aws-cdk-lib\",\n                    \"npm install -g aws-cdk\",\n                ],\n                commands=[\n                    \"cdk synth\",\n                ],\n            ),\n            pipeline_name=f\"{context.stage}-{context.name}-Pipeline\",\n        )\n\n        pipeline.add_stage(DeployStage(self, context))\n</code></pre> <p>On line 10, the <code>context</code> decorator assigns the stage name as <code>Dev</code> and configures the use of resources tagged as <code>dev</code> in the <code>cdk.json</code> file. Moreover, it imports some additional configuration variables from the <code>cdk.json</code> file, assigning them to the argument named <code>context</code>.</p> cdk.json<pre><code>    \"region\": \"us-east-2\",\n    \"account\": \"\",\n    \"name\": \"Lambda-Forge-Demo\",\n    \"repo\": {\n      \"owner\": \"$GITHUB-OWNER\",\n      \"name\": \"$GITHUB-REPO\"\n    },\n    \"bucket\": \"\",\n    \"coverage\": 80,\n    \"dev\": {\n        \"arns\": {}\n    },\n    \"staging\": {\n        \"arns\": {}\n    },\n    \"prod\": {\n        \"arns\": {}\n    }\n</code></pre> <p>Additionally, we incorporate the source code from the <code>dev</code> branch hosted on GitHub into the pipeline. Subsequently, we finalize the deployment of the Lambda functions by activating the <code>DeployStage</code>.</p>"},{"location":"home/page5/#development-pipeline-workflow","title":"Development Pipeline Workflow","text":"<p>As the deployment of the Development Environment has been covered in previous sections, we'll not revisit those steps here. However, the diagram below succinctly illustrates the pipeline configuration established within the AWS CodePipeline.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline[Update Pipeline]     UpdatePipeline --&gt; Assets     Assets --&gt; Deployment"},{"location":"home/page5/#staging-environment","title":"Staging Environment","text":"<p>The <code>Staging</code> environment serves as a near-replica of the production environment, enabling thorough testing and quality assurance processes to catch any bugs or issues before they reach the end-users.</p>"},{"location":"home/page5/#configuring-the-staging-environment","title":"Configuring the Staging Environment","text":"<p>Let's take a deeper look in the staging configuration file.</p> infra/stacks/staging_stack.py<pre><code>import aws_cdk as cdk\nfrom aws_cdk import pipelines as pipelines\nfrom aws_cdk.pipelines import CodePipelineSource\nfrom constructs import Construct\nfrom lambda_forge import Steps, context\n\nfrom infra.stages.deploy import DeployStage\n\n\n@context(stage=\"Staging\", resources=\"staging\")\nclass StagingStack(cdk.Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n        super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs)\n\n        source = CodePipelineSource.git_hub(\n            f\"{context.repo['owner']}/{context.repo['name']}\", \"staging\"\n        )\n\n        pipeline = pipelines.CodePipeline(\n            self,\n            \"Pipeline\",\n            synth=pipelines.ShellStep(\n                \"Synth\",\n                input=source,\n                install_commands=[\n                    \"pip install lambda-forge aws-cdk-lib\",\n                    \"npm install -g aws-cdk\",\n                ],\n                commands=[\n                    \"cdk synth\",\n                ],\n            ),\n            pipeline_name=f\"{context.stage}-{context.name}-Pipeline\",\n        )\n\n        steps = Steps(self, context, source)\n\n        # pre\n        unit_tests = steps.run_unit_tests()\n        coverage = steps.run_coverage()\n        validate_docs = steps.validate_docs()\n        validate_integration_tests = steps.validate_integration_tests()\n\n        # post\n        generate_docs = steps.generate_docs()\n        integration_tests = steps.run_integration_tests()\n\n        pipeline.add_stage(\n            DeployStage(self, context),\n            pre=[\n                unit_tests,\n                coverage,\n                validate_integration_tests,\n            ],\n            post=[integration_tests],\n        )\n</code></pre> <p>Similar to the <code>Dev</code> environment, this environment is named <code>Staging</code>, with resources designated as <code>staging</code> in the <code>cdk.json</code> file. We also integrate the source code from the <code>staging</code> branch on GitHub into the pipeline. However, in contrast to Dev, the Staging environment incorporates stringent quality assurance protocols prior to deployment.</p> <p>Before deploying the functions, we execute all unit tests specified in the <code>unit.py</code> files. Additionally, we ensure that the code coverage percentage exceeds the threshold set in the <code>cdk.json</code> file. We also verify that every function connected to the API Gateway is subjected to at least one integration test, identified by the custom <code>pytest.mark.integration</code> decorator.</p> <p>Once all functions have been successfully deployed, we proceed to conduct integration tests as detailed in the <code>integration.py</code> files. Essentially, this procedure entails dispatching an HTTP request to each of the newly deployed functions and ensuring they respond with a 200 status code.</p> <p>Initially, the project was initiated with the <code>--no-docs</code> flag, resulting in the <code>validate_docs</code> and <code>generate_docs</code> steps being created but not integrated into the pipeline. We will delve into these steps in greater depth, exploring their functionality and potential benefits in the next section.</p>"},{"location":"home/page5/#deploying-the-staging-environment","title":"Deploying the Staging Environment","text":"<p>First let's create and push the current code to a new branch called <code>staging</code>.</p> <pre><code># Stage your changes\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Deploying the Staging Environment\"\n\n# Create/switch to 'staging' branch.\ngit checkout -b staging\n\n# Push 'staging' to remote.\ngit push origin staging\n</code></pre> <p>Next, let's deploy the staging environment with CDK, adhering to the naming conventions established by Forge:</p> <pre><code>cdk deploy Staging-Lambda-Forge-Demo-Stack\n</code></pre> <p>This command initiates the deployment process. Shortly, AWS CodePipeline will integrate a new pipeline, specifically tailored for the staging environment.</p> <p></p> <p>The pipeline's configuration within AWS CodePipeline is depicted below, showcasing the streamlined workflow from source code to deployment:</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline[Update Pipeline]     UpdatePipeline --&gt; Assets     Assets --&gt; UnitTests[Unit Tests]     Assets --&gt; Coverage     Assets --&gt; ValidateIntegrationTests[Validate Integration Tests]     UnitTests --&gt; Deploy     Coverage --&gt; Deploy     ValidateIntegrationTests --&gt; Deploy     Deploy --&gt; IntegrationTests[Integration Tests] <p>The first deployment of the Staging Pipeline often results in failure, a situation that might seem alarming but is actually expected due to the sequence in which components are deployed and tested.</p> <p>This phenomenon occurs because the integration tests are set to execute immediately after the deployment phase. However, during the first deployment, the BASE URL vital for these tests hasn't been established since it's the inaugural setup of the Staging environment. Consequently, this leads to the failure of the <code>Integration_Test</code> phase.</p> <p></p> <p>Note that the failure arises after the deployment phase, indicating that the Lambda functions have been successfully deployed.</p> <p>To address this challenge, the solution involves a simple manual step in the AWS Lambda console. Specifically, you'll need to locate the function named <code>Staging-Lambda-Forge-Demo-HelloWorld</code>.</p> <p></p> <p>Upon finding the function, proceed to <code>Configurations -&gt; Triggers</code>. Here, you'll discover the url that was generated for this function during the deployment.</p> <p></p> <p>For this tutorial, the complete url is:</p> <ul> <li>https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world</li> </ul> <p>Given this, the BASE URL can be deduced as the portion of the URL preceding the <code>/hello_world</code> endpoint, which in this case is: <code>https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging</code>.</p> <p>This BASE URL must then be incorporated into your <code>cdk.json</code> configuration file under the <code>base_url</code> key. This adjustment ensures that all integration tests can interact with the staging environment seamlessly for automated testing.</p> cdk.json<pre><code>    \"bucket\": \"\",\n    \"coverage\": 80,\n    \"base_url\": \"https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging\"\n</code></pre> <p>With this setup, your integration tests are now aligned with the staging environment, facilitating a smoother and reliable testing phase.</p> <p>Finally, commit your changes and push the updated code to GitHub once again. Following these adjustments, the pipeline should successfully complete its run.</p>"},{"location":"home/page5/#production-environment","title":"Production Environment","text":"<p>The <code>Production</code> environment represents the phase where the tested and stable version of the software is deployed. This version is accessible to end-users and operates within the live environment. It is imperative that this stage remains the most safeguarded, permitting only fully vetted and secure code to be deployed. This precaution helps in minimizing the risk of exposing end-users to bugs or undesirable functionalities, ensuring a seamless and reliable user experience.</p>"},{"location":"home/page5/#configuring-the-production-environment","title":"Configuring the Production Environment","text":"<pre><code>import aws_cdk as cdk\nfrom aws_cdk import pipelines\nfrom aws_cdk.pipelines import CodePipelineSource\nfrom constructs import Construct\nfrom lambda_forge import Steps, context, create_context\n\nfrom infra.stages.deploy import DeployStage\n\n\n@context(\n    stage=\"Prod\",\n    resources=\"prod\",\n    staging=create_context(stage=\"Staging\", resources=\"staging\"),\n)\nclass ProdStack(cdk.Stack):\n    def __init__(self, scope: Construct, context, **kwargs) -&gt; None:\n        super().__init__(scope, f\"{context.stage}-{context.name}-Stack\", **kwargs)\n\n        source = CodePipelineSource.git_hub(\n            f\"{context.repo['owner']}/{context.repo['name']}\", \"main\"\n        )\n\n        pipeline = pipelines.CodePipeline(\n            self,\n            \"Pipeline\",\n            synth=pipelines.ShellStep(\n                \"Synth\",\n                input=source,\n                install_commands=[\n                    \"pip install lambda-forge aws-cdk-lib\",\n                    \"npm install -g aws-cdk\",\n                ],\n                commands=[\n                    \"cdk synth\",\n                ],\n            ),\n            pipeline_name=f\"{context.stage}-{context.name}-Pipeline\",\n        )\n\n        steps = Steps(self, context.staging, source)\n\n        # pre\n        unit_tests = steps.run_unit_tests()\n        coverage = steps.run_coverage()\n        validate_docs = steps.validate_docs()\n        validate_integration_tests = steps.validate_integration_tests()\n\n        # post\n        integration_tests = steps.run_integration_tests()\n\n        pipeline.add_stage(\n            DeployStage(self, context.staging),\n            pre=[\n                unit_tests,\n                coverage,\n                validate_integration_tests,\n            ],\n            post=[integration_tests],\n        )\n\n        # post\n        generate_docs = steps.generate_docs()\n\n        pipeline.add_stage(\n            DeployStage(self, context),\n            post=[],\n        )\n</code></pre> <p>This environment is named <code>Prod</code> and the resources used are provenient from the <code>prod</code> key in the <code>cdk.json</code> file. Additionally, the <code>main</code> branch on GitHub is being used to trigger the pipeline. Given the critical need for security and integrity in production, we replicate the staging environment, applying all tests and safeguards again before deploying the production stage. This ensures that any changes meet our high quality standards before production deployment, effectively protecting against vulnerabilities and ensuring a stable user experience.</p>"},{"location":"home/page5/#deploying-the-production-environment","title":"Deploying the Production Environment","text":"<p>Firstly, commit and push your code to a new branch named <code>main</code> on GitHub</p> <pre><code># Stage your changes\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Deploying the Production Environment\"\n\n# Create/switch to 'main' branch.\ngit checkout -b main\n\n# Push 'main' to remote.\ngit push origin main\n</code></pre> <p>Following the branch setup, deploy your staging environment using the AWS CDK, adhering to the naming conventions provided by Forge.</p> <pre><code>cdk deploy Prod-Lambda-Forge-Demo-Stack\n</code></pre> <p>Executing this command initiates the creation of a new pipeline in AWS CodePipeline, designed to automate your deployment process.</p> <p></p> <p>The following diagram visually represents the configuration established in AWS CodePipeline.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline[Update Pipeline]     UpdatePipeline --&gt; Assets     Assets --&gt; UnitTests[Unit Tests]     Assets --&gt; Coverage     Assets --&gt; ValidateIntegrationTests[Validate Integration Tests]     UnitTests --&gt; DeployStaging[Deploy Staging]     Coverage --&gt; DeployStaging     ValidateIntegrationTests --&gt; DeployStaging     DeployStaging --&gt; IntegrationTests[Integration Tests]     IntegrationTests --&gt; DeployProduction[Deploy Production] <p>Upon the successful completion of the pipeline execution, you'll be able to observe a new Lambda function ready and deployed within your AWS Lambda console </p> <p>To verify the url created, navigate to the newly deployed Lambda function in the AWS Lambda console. Within the function, proceed to <code>Configurations -&gt; Triggers</code>. Here, you'll find the URL for the new endpoint that has been activated as part of the deployment process.</p> <p>For this tutorial, the endpoint URL provided is:</p> <ul> <li>https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/hello_world</li> </ul>"},{"location":"home/page5/#overview","title":"Overview","text":"<p>By adhering to the instructions outlined in this tutorial, you are now equipped with three distinct CI/CD pipelines. Each pipeline corresponds to a specific stage of the development lifecycle, directly linked to the <code>dev</code>, <code>staging</code>, and <code>main</code> branches in your GitHub repository.</p> <p>These pipelines ensure that changes made in each branch are automatically integrated and deployed to the appropriate environment, streamlining the process from development through to production.</p> <p></p> <p>Furthermore, you have deployed three unique functions, each corresponding to a different environment:</p> <ul> <li>Dev: https://gxjca0e395.execute-api.us-east-2.amazonaws.com/dev/hello_world</li> <li>Staging: https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/hello_world</li> <li>Prod: https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/hello_world</li> </ul> <p>Each link directs you to the corresponding function deployed within its respective environment, demonstrating the successful separation and management of development, staging, and production stages through your CI/CD workflows.</p> <p>Congratulations! \ud83c\udf89 You've successfully deployed your Lambda function across three different environments using Lambda Forge! \ud83d\ude80</p>"},{"location":"home/page6/","title":"Generating Docs With Swagger and ReDoc","text":""},{"location":"home/page6/#documenting-a-lambda-function","title":"Documenting a Lambda Function","text":"<p>Inside each <code>main.py</code> file, you should include the <code>Input</code> and <code>Output</code> dataclasses that are going to be the entrypoint for generating the docs. Case you have an endpoint that's expecting a path parameter, you can also include it in the <code>Path</code> dataclass.</p> <p>The code snippet below demonstrates all the types of data you can expect to work with, including simple data types, lists, custom objects, optional fields, and literal types, offering a clear understanding of the input and output contracts for the API.</p> <pre><code>from dataclasses import dataclass\nfrom typing import List, Optional, Literal\n\n# Define a dataclass for path parameters, useful for API endpoints requiring parameters within the URL path.\n@dataclass\nclass Path:\n    id: str\n\n# A custom object class that represents a complex type with multiple fields.\n@dataclass\nclass Object:\n    a_string: str\n    an_int: int\n\n# The input data class represents the expected structure of the request payload.\n@dataclass\nclass Input:\n    a_string: str  # A simple string input\n    an_int: int  # A simple integer input\n    a_boolean: bool  # A boolean value\n    a_list: List[str]  # A list of strings\n    an_object: Object  # An instance of the custom 'Object' class defined above\n    a_list_of_object: List[Object]  # A list containing instances of 'Object'\n    a_literal: Literal[\"a\", \"b\", \"c\"]  # A literal type, restricting values to 'a', 'b', or 'c'\n    an_optional: Optional[str]  # An optional string, which can be either a string or None\n\n# The output data class represents the endpoint's output.\n@dataclass\nclass Output:\n    pass # No fields are defined, implying the output is empty.\n</code></pre>"},{"location":"home/page6/#setting-up-a-s3-bucket-for-documentation","title":"Setting Up a S3 Bucket for Documentation","text":"<p>Create an Amazon S3 bucket to serve as the primary storage for your documentation files. Follow these steps to create your S3 bucket:</p> <ol> <li>Access the AWS Management Console: Open the Amazon S3 console at https://console.aws.amazon.com/s3/.</li> <li>Create a New Bucket: Click on the \"Create bucket\" button. It's important to note that each bucket's name must be globally unique across all of Amazon S3.</li> <li>Set Bucket Name: Choose a unique and descriptive name for your bucket. This name will be crucial for accessing your documentation files. Remember, once a bucket name is set, it cannot be changed.</li> <li>Choose a Region: Select an AWS Region for your bucket. Choose the same region defined in your <code>cdk.json</code>.</li> <li>Configure Options: You may leave the default settings or configure additional options like versioning, logging, or add tags according to your needs.</li> <li>Review and Create: Before creating the bucket, review your settings. Once everything is confirmed, click \"Create bucket\".</li> </ol> <p>Once the bucket is created, update your <code>cdk.json</code> file with the bucket's name as shown below:</p> cdk.json<pre><code>...\n\"region\": \"us-east-2\",\n\"account\": \"\",\n\"name\": \"Lambda-Forge-Demo\",\n\"repo\": {\n    \"owner\": \"$GITHUB-USER\",\n    \"name\": \"$GITHUB-REPO\"\n},\n\"bucket\": \"$S3-BUCKET-NAME\",\n\"coverage\": 80,\n...\n</code></pre>"},{"location":"home/page6/#setting-up-documentation-endpoints","title":"Setting Up Documentation Endpoints","text":"<p>To activate docs generation, navigate to the <code>deploy.py</code> file located at <code>infra/stages/deploy.py</code>. Configure your endpoints as illustrated in the following example.</p> infra/stages/deploy.py<pre><code>import aws_cdk as cdk\nfrom constructs import Construct\n\nfrom infra.stacks.lambda_stack import LambdaStack\n\n\nclass DeployStage(cdk.Stage):\n    def __init__(self, scope: Construct, context, **kwargs):\n        super().__init__(scope, context.stage, **kwargs)\n\n        lambda_stack = LambdaStack(self, context)\n\n        # Sets up a Swagger-based public endpoint at /docs\n        lambda_stack.services.api_gateway.create_docs(authorizer=None)\n\n        # Establishes a Swagger-based private endpoint at /docs/private with the 'secret' authorizer\n        lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private\")\n\n        # Configures a Redoc-based public endpoint at /docs/redoc\n        lambda_stack.services.api_gateway.create_docs(authorizer=None, endpoint=\"/docs/redoc\", redoc=True)\n\n        # Sets up a Redoc-based private endpoint at /docs/private/redoc with the 'secret' authorizer\n        lambda_stack.services.api_gateway.create_docs(authorizer=\"secret\", endpoint=\"/docs/private/redoc\", redoc=True)\n</code></pre> <p>This configuration enables both public and private documentation endpoints using Swagger and Redoc, making your API's documentation accessible and versatile.</p>"},{"location":"home/page6/#configuring-the-pipelines-for-docs-generation","title":"Configuring the Pipelines for Docs Generation","text":"<p>Given that the development stage is designed for fast deployment and serves as a sandbox environment, Lambda Forge does not automatically generate documentation for the dev environment. However, if you wish to include docs generation in your development workflow, replicating the following steps for the dev environment should effectively enable this functionality.</p> <p>Because the project was started with the <code>--no-docs</code> flag, it currently lacks the <code>validate_docs</code> and <code>generate_docs</code> steps in both the Staging and Production pipelines.</p> <p>In essence, the <code>validate_docs</code> step ensures that all files intended for documentation are correctly configured with the necessary data classes. This step checks for completeness and accuracy in the documentation's underlying structure. On the other hand, the <code>generate_docs</code> step takes on the role of creating the documentation artifact itself and deploying it to the S3 bucket configured on the <code>cdk.json</code> file.</p> <p>To incorporate docs generation into your project, you'll need to modify your stack configurations. Specifically, you should enable the <code>validate_docs</code> and <code>generate_docs</code> steps within your CI/CD pipeline configurations for both Staging and Production environments.</p>"},{"location":"home/page6/#configuring-the-staging-pipeline","title":"Configuring the Staging Pipeline","text":"<p>To turn on documentation generation for the staging environment, add <code>validate_docs</code> in the pipeline's pre-execution phase and <code>generate_docs</code> post-deployment.</p> infra/stacks/staging_stack.py<pre><code>    # pre\n    unit_tests = steps.run_unit_tests()\n    coverage = steps.run_coverage()\n    validate_docs = steps.validate_docs()\n    validate_integration_tests = steps.validate_integration_tests()\n\n    # post\n    generate_docs = steps.generate_docs()\n    integration_tests = steps.run_integration_tests()\n\n    pipeline.add_stage(\n        DeployStage(self, context),\n        pre=[\n            unit_tests,\n            coverage,\n            validate_integration_tests,\n            validate_docs, # Validate docs enabled\n        ],\n        post=[integration_tests, generate_docs], # Generate docs enabled\n    )\n</code></pre> <p>The pipeline configuration will change for the staging environment. The following diagram illustrates the adjusted setup.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline[Update Pipeline]     UpdatePipeline --&gt; Assets     Assets --&gt; UnitTests[Unit Tests]     Assets --&gt; Coverage     Assets --&gt; ValidateDocs[Validate Docs]     Assets --&gt; ValidateIntegrationTests[Validate Integration Tests]     UnitTests --&gt; Deploy     Coverage --&gt; Deploy     ValidateDocs --&gt; Deploy     ValidateIntegrationTests --&gt; Deploy     Deploy --&gt; IntegrationTests[Integration Tests]     Deploy --&gt; GenerateDocs[Generate Docs]"},{"location":"home/page6/#configuring-the-production-pipeline","title":"Configuring the Production Pipeline","text":"<p>Similarly for the Production stack, ensure that <code>validate_docs</code> and <code>generate_docs</code> are enabled.</p> infra/stacks/prod_stack.py<pre><code>    # pre\n    unit_tests = steps.run_unit_tests()\n    coverage = steps.run_coverage()\n    validate_docs = steps.validate_docs()\n    validate_integration_tests = steps.validate_integration_tests()\n\n    # post\n    integration_tests = steps.run_integration_tests()\n\n    pipeline.add_stage(\n        DeployStage(self, context.staging),\n        pre=[\n            unit_tests,\n            coverage,\n            validate_integration_tests,\n            validate_docs, # Validate docs enabled\n        ],\n        post=[integration_tests],\n    )\n\n    # post\n    generate_docs = steps.generate_docs()\n\n    pipeline.add_stage(\n        DeployStage(self, context),\n        post=[generate_docs], # Generate docs enabled\n    )\n</code></pre> <p>The pipeline configuration will also change change for the staging environment. The following diagram illustrates the adjusted setup.</p> graph TD;     Source --&gt; Build;     Build --&gt; UpdatePipeline[Update Pipeline]     UpdatePipeline --&gt; Assets     Assets --&gt; UnitTests[Unit Tests]     Assets --&gt; Coverage     Assets --&gt; ValidateDocs[Validate Docs]     Assets --&gt; ValidateIntegrationTests[Validate Integration Tests]     UnitTests --&gt; DeployStaging[Deploy Staging]     Coverage --&gt; DeployStaging     ValidateDocs --&gt; DeployStaging     ValidateIntegrationTests --&gt; DeployStaging     DeployStaging --&gt; IntegrationTests[Integration Tests]     IntegrationTests --&gt; DeployProduction[Deploy Production]     DeployProduction --&gt; GenerateDocs[Generate Docs]"},{"location":"home/page6/#deploying-the-docs","title":"Deploying the Docs","text":"<p>At this point, we have all the necessary components to automatically generate our docs.</p> <p>To proceed, commit your changes and push them to GitHub using the following commands:</p> <pre><code># Send your changes to stage\ngit add .\n\n# Commit with a descriptive message\ngit commit -m \"Activating docs for Staging and Production environments\"\n\n# Push changes to the 'dev' branch\ngit push origin dev\n\n# Merge 'dev' into 'staging' and push\ngit checkout staging\ngit merge dev\ngit push origin staging\n\n# Finally, merge 'staging' into 'main' and push\ngit checkout main\ngit merge staging\ngit push origin main\n</code></pre> <p>After the pipeline completes successfully, the documentation for your API's endpoints will be available through the URLs set up in the <code>DeployStage</code> class. This documentation offers detailed insights into the endpoints, including their request formats, response structures, and available query parameters.</p> <p>For easy access, the documentation for public endpoints in each environment is provided at:</p> <ul> <li> <p>Staging Environment:</p> </li> <li> <p>https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs</p> </li> <li> <p>https://8kwcovaj0f.execute-api.us-east-2.amazonaws.com/staging/docs/redoc</p> </li> <li> <p>Production Environment:</p> </li> <li>https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs</li> <li>https://s6zqhu2pg1.execute-api.us-east-2.amazonaws.com/prod/docs/redoc</li> </ul> <p>Accessing the private endpoints, <code>/docs/private</code> for Swagger and <code>/docs/private/redoc</code> for Redoc, necessitates the inclusion of the security token generated by the <code>secret</code> authorizer, as specified in the authorizers section.</p>"},{"location":"home/page7/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 Guilherme Alves Pimenta</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"}]}